it's time to build a fully fledged back-end server in go from scratch on
0:05
our local machines the purpose of the server will be to aggregate data from RSS feeds if you're not familiar with
0:11
RSS it's a protocol that makes Distributing things like podcasts and blog posts really easy so what our
0:17
server will allow users to do is add different RSS feeds to its database and
0:22
then it will go automatically collect all of the posts from those feeds and download them and save them in the
0:29
database so that we can then view them later before we get started there are four things you're going to need the first is
0:37
a basic understanding of SQL the language that's most often used to query relational databases if you're not
0:43
familiar with SQL yet that's okay I've got a full course on SQL I'll link that down in the description below go watch
0:50
that then come back here if you're not familiar with SQL number two is you're going to need a text editor and a
0:56
command line I'm using vs code and zsh here in the video so you'll see me using that feel free to go download those if
1:03
you want to try them out but you can use whatever text editor you want as long as it can edit your files and you have
1:09
access to a terminal to run commands number three is the go programming language itself if you don't have that
1:15
you can go download it I will link uh the download page down in the description below and I'd also recommend
1:20
just installing the go plug-in um if you're in vs code there is an official go plug-in Go download that
1:27
it'll make your life easier with syntax highlighting and formatting and that sort of thing number four the last thing
1:32
you'll need is an http client so an H D client will allow you to make get and post and put requests
1:39
into the web server that we're building we'll need that for testing I use the Thunder client it's a vs code extension
1:47
but you can use anything you like even curl on the command line or Postman insomnia there's tons of choices Google
1:52
HTTP clients or if you don't already have a preference again if you're in vs code I'd recommend the Thunder client
1:59
extension now that you have all of those tools installed and hopefully working let's jump into the project the first
2:06
thing we're going to need is just a main.go file we'll create our entry point it will be a part of the main
2:12
package and it will need a main function so Funk main takes no arguments return
2:18
those returns no parameters and for now let's just print hello world make sure we can kind of build and run this
2:24
program we're going to need to initialize a new module for our project so I'm going to do go mod init
2:32
and I like to name my modules after their remote path right where they will exist
2:37
um kind of out on the internet so in my case I use github.com my GitHub uh
2:43
namespace which is wagslane Slash the name of this repository which again is
2:48
where I'll be keeping this code on GitHub so I highly recommend you keep track of this code in GitHub this entire
2:55
project should be checked into git and uploaded to GitHub or gitlab or whatever
3:00
you prefer so I'm going to name this repository RSS aggregator RSS AG so we'll go ahead
3:08
and create that that will create this new go module and now that that's ready we should be
3:15
able to just go build and execute the new
3:20
um binary or the new executable that will be created from go buildings I'll just run it once so you can see
3:26
that created this new binary file here in my current directory and so from now
3:31
on I'll be running this command go build and Dot slash
3:36
RSS AG so that will build and run and we got hello world back so we're
3:41
good to go like I said before we're going to be building this project with Git and storing our code in Source
3:47
control as we go so I'm going to go ahead and create a new git repository and in vs code it
3:53
highlights all the code that has changed but not yet been committed to Source control here in green but I don't want
3:59
all of this in my source control the dot vs code file is configurations for my editor that's a personal thing that
4:05
doesn't need to be in the project itself same with the RSS AG binary we don't want to commit the binary that we're
4:12
building or the executable file that we're building we just want to commit our source code so I'm going to create a
4:18
new DOT get ignore file and we're going to ignore the dot vs
4:23
code folder and the RSS AG binary
4:28
and next um we're going to add all of the secrets
4:34
the like configuration secrets for our project in a DOT EnV file and read them
4:41
out of the file itself so for example one of the configuration variables that we're going to set is the port that the
4:47
server will run on so I'm going to set that port to 8000.
4:53
and I'm going to also ignore that dot EnV file in the git ignore
4:59
again because configuration data is something kind of local to my machine in production this port might be something
5:05
different so we don't need to commit this file to Source control um but I do want it here in my repo now
5:12
at this point I do want to pause and say if you have no idea what a port is or
5:17
you have no idea what HTTP requests are or rest apis are um we're going to move fairly quickly in
5:24
this project so if you're not familiar with that stuff again I will link down in the description below my HTTP course
5:30
that would be a good one to go brush up on before working on this project so now we need a way to read this port
5:38
variable into our program so that we can use it and the go standard library has a
5:43
built-in function called os.getn so it's a an exported function
5:48
called get end from the OS package and we can get the value of a variable by
5:54
its key so in this case the key is port and we'll get back a
6:00
Port string and then for now let's just uh let's
6:05
just say if Port string equals the empty string then we'll say
6:11
log dot fatal so log.fatal will exit the program immediately with arrow code 1
6:18
and a message and we'll say port is not found in the environment
6:27
otherwise we'll say port
6:33
and we'll print the port string okay cool let's go ahead and run that
6:44
word is not found in the environment okay so the problem here is that Port the environment variable doesn't exist
6:51
in my current shell session if I wanted to add it I could run in my in my
6:56
command line export Port equals 8000. and then run this again
7:03
and then we get Port equals eight thousand the problem is I don't want to manually set this environment variable
7:10
every time I work on my server I want to pull it from this file so we're going to
7:15
use a package that allows us to grab environment variables from a DOT ENB file and it is this package here
7:23
github.com joho slash go.env that's also the URL of the library you can paste
7:30
that into your browser go check it out but we're just going to install it here locally
7:36
and that will add it here to our go.mod and then I'm going to run go mod vendor
7:43
to copy that code here into my my vendor folder we get a kind of a
7:51
local copy of that we'll run um here we're going to need to actually use
7:58
it so we do go.env.load and by default load loads the dot EnV
8:07
file I think we can also optionally pass in dot EnV as the file path
8:14
and what's uh what's my error here could not import no required module provides
8:20
oh that's let's go mod tidy that should clean up my imports okay perfect
8:28
and do I need to do anything else what are we getting here could not import no
8:34
required module I should probably go mod vendor again
8:39
that should pull in the code okay so you can see we've kind of imported and downloaded all of that code from the
8:45
package and now I'm not getting air any errors in my console okay so this will take the
8:51
environment variables from my DOT EnV file and pull them into my current environment so then I can use os.getendv
8:58
to load the variable so to test that let's go ahead and change uh the port to
9:03
8080. and rerun the server
9:09
still says eight thousand so something went wrong maybe I'm misremembering how to use this uh use this package
9:16
let's try just go dot into that load still port 80.
9:22
what am I doing wrong does it not overwrite you know what it might not overwrite my current session
9:28
I'm going to kill my current sell session shell session and create a new one
9:33
and then we'll do this again so now I won't have that exported 8000 that I had
9:38
in my terminal um run that again okay cool so now it's pulling it from
9:44
the file because I don't already have it defined in my shell session now I want to take just a second and point out
9:50
there are text instructions for this entire project over on boot.dev and I'll
9:55
link that down in the description below we're going to be using a lot of text uh you know code Snippets from those text
10:02
instructions and they'll be easier to kind of copy and paste and grab from boot.dev directly then trying to you
10:08
know retype what you're seeing me type here on the screen now we're going to actually spin up our server and we're
10:13
going to be using the Qi router to do it it's a third-party router very lightweight built on top of kind of the
10:20
same way that the standard library in go does http routers and so let's go ahead and install those now we'll do go get
10:27
github.com go Dash chai slash chai or chi I always struggle to pronounce that
P2. Chi router
10:33
one we'll install that and we'll install this the cores package from the same uh
10:40
the same name space the G namespace next we'll create a new router so I'll do
10:45
router colon equals Qi dot new router fact I should probably go mod vendor
10:55
have it there and then I'll do another go mod tidy and go modbender to bring it in
11:02
cool so this creates a new router object next we'll connect up this router to an
11:08
http.server so that serve colon equals a pointer to an HTTP dot server
11:15
and a server needs a Handler which will be the router itself
11:21
and we also need a or an address which
11:26
is just a colon plus that Port string so
11:32
in this case it'll be colon you know 8080. cool and then we can call http.listen and serve
11:41
or sorry not http.listening server we want to call it on the server object so serve.list.serve
11:46
cool and before we call that in fact I think that returns an error so let's capture that error
11:52
say if error is not equal nil log dot fatal
11:58
as in the error is a message okay listen and serve will block so when
12:04
we get to line 30 our our code basically just stops right here and starts handling HTTP requests
12:11
if anything goes wrong in the process of handling those requests then an error
12:17
will be returned and we'll you know log it and exit the program but kind of the happy path for our code is that you know
12:24
nothing should ever be returned from listen and serve because our server is just going to run forever before we run
12:30
this let's just add one more kind of logging statement we'll do log dot print line actually let's do printf
12:38
and we'll say server starting on Port
12:43
percent V and we'll pass in that Port string
12:48
okay cool with that let's go ahead and build and run again so go build
12:56
see what we get Hello World server starting on Port 8080. I should probably remove I should
13:03
probably remove that hello world at this point now that we have a running server let's go ahead and test it so I'm over here in
13:09
the Thunder client tab again because I'm using the thunderclient plugin and I'm going to click new request
13:15
and we're going to make a request to http colon slash localhost right we want
13:21
to make a request to our own machine on the port that we're running on which I believe is 8080.
13:27
okay with that let's go ahead and start up our server
13:36
okay server starting on port 8080 so it should be good now you'll see I don't have a new prompt because my my server
13:42
is still running if I send this get request perfect we get a 404 that's
13:47
exactly what we'd expect because remember in our code we haven't actually set up any handlers or anything we just
13:53
have a server running so we're getting a 404 because we're trying to hit a path in this case the root path and it
14:00
doesn't have anything any logic there to handle that code if we killed our server
14:05
and ran it again we just get the connection refused I've configured my thunderclient to actually store all of
14:13
my tests or my HTTP requests as plain text here in the working directory but I
14:19
don't want those going into my source control so I'm going to go ahead and add that to the git ignore Thunder Dash
14:24
tests will ignore everything in there next let's add a course configuration to
14:30
our router so this is so that people can make requests to our server from a browser and we're going to be using some
14:35
fairly permissive configurations here and we'll use router dot use and then
14:41
we'll pass in this cores.handler configuration this comes from that course package that we
14:47
installed earlier and let me see what am I doing here we need one more parenthesis there
14:53
and then I'm going to go ahead and vendor this as well so go mod tidy
14:58
go mod vendor cool so we should have all of that code
15:05
here in our vendor folder as well I'm not going to go too in depth on exactly
15:12
what cores are you can definitely go look that up but just to give you a high level overview this configuration is
15:17
essentially telling our server to send a bunch of uh extra HTTP headers in our
15:23
responses that will tell browsers hey we allow you to send uh you know requests
15:30
to http or https versions We allow you to use these methods we allow you to send any headers it's just a way to say
15:37
hey we're going to allow you to do basically whatever you want there are ways you can tighten up this
15:43
configuration for security purposes but for now we're just going to be running our project on our local machine so
15:48
we're going to just open it up make it permissive to avoid any sort of uh kind of weird testing issues if we try to
15:55
connect to our server through a browser
16:00
rest API which means all of the request bodies coming in and response bodies going back will have a Json format so
16:08
let's create a little helper function that will make it easier to send Json responses so I'm going to create a new
16:14
file called json.go it's going to be in the main package and the function signature is going to
16:20
look like this so we've got a function we're calling it respond with Json it takes as input a response writer this is
16:26
the same HTTP response writer that HTTP handlers in go use it's exposed by the
16:31
standard Library it will take a code so this is the status code we're going to respond with
16:36
and it will take an interface which is just something that we can Marshal to a
16:42
Json structure the first thing the function will do is Marshal the payload into a Json object or a Json string and
16:49
the way we do that is with the standard Library so we need data and error equals Json dot Marshall
16:56
and we pass in the payload so this function will attempt to Marshall whatever it's given into a Json
17:04
string and it will return it as bytes and the reason it returns it is bytes is so that we can write it in a binary
17:09
format directly to the HTTP response which is pretty convenient that fails for whatever reason
17:16
then what we'll do is we'll write a header to the response
17:22
and we'll use status code 500 we'll say something went wrong on our end right internal service error or internal
17:28
server error um and then we'll just return from the function and actually if something goes
17:33
wrong we should probably log it as well on the server side so that we can see our own logs and see hey we tried to do
17:39
something and it broke uh so we'll do log dot print line
17:45
failed to Marshall Json response
17:51
and let's print the response or let's print what we tried to Marshall
17:56
that's probably more more interesting we'll use printfs that we can interpolate that value there next
18:03
we're going to need to add a header to the response to say that we're responding with Json data so we'll do w
18:08
dot right header and or not right header I'm sorry w dot
18:15
headers dot is it header
18:21
dot add and we want to add the content type key
18:26
so content type and the key will be applica or the value of the application Json
18:33
so this adds a response header to the HTTP request saying hey we're responding with a content type of application slash
18:40
Json which is the standard and a value for Json responses and then we should be
18:45
able to write the status code so we do w dot right header 200 so everything went
18:53
well and then we need to write the data itself so w dot write and pass in the Json data this will
19:00
write the response body now that we have a way to respond with some Json data let's create an HTTP Handler that does
19:07
that so we'll do handlers or Handler readiness
19:14
again this will be in the main package and we're going to create a new function
19:20
called Handler Readiness and this is a very specific function signature this is
19:25
the function signature that you have to use if you want to Define an HTTP Handler in the way that the go standard
19:32
Library expects so it always takes a response writer as the first parameter and a pointer to an HTTP request as the
19:40
second parameter and then in the body of this Handler we can just call our respond with Json function
19:46
so we'll say respondus Json we'll pass in that HTTP response writer
19:52
we want to respond with a 200 status code and some
19:58
some response payload in this case all we care about is the 200 okay status
20:04
code so I'm actually just going to respond with an empty struct which should Marshal to kind of an empty Json object
20:10
and now that I'm writing this I realize that we actually made a mistake or I made a mistake in the Json uh response
20:17
Json code we should pass in uh we should use the passed in Response Code instead
20:22
of hard coding the 200. so if everything goes right we'll use the code given okay
20:27
now with that we need to hook up our Handler Now using the chi router what we
20:33
do is we hook up a an HTTP Handler which is this function to a specific HTTP
20:38
method and path okay so the way we're going to do that is I'm going to create a new router so V1 router
20:45
and we'll use that same Chi dot new router to do it and I'm going to specify
20:52
V1 router dot handle handle Funk excuse me
20:58
I want to handle the slash ready path and I want to handle it with this
21:04
Handler Readiness function okay so we're we're connecting the Handler Readiness function to the slash
21:11
ready path and the reason I created this new V1 router is because I'm going to mount that so I can do router.mount
21:21
to the slash V1 path okay so I'm nesting a V1 router under
21:28
the slash B1 path and I'm hooking up the Readiness Handler at the slash ready path so the full path for this request
21:36
will be slash B1 slash ready and that's just so that if we make breaking changes in the future we can kind of have two
21:43
different handlers one under version one and one under version two for our rest API this is a fairly standard practice
21:49
and actually I'm going to name this path health Health Z that's just a habit uh that I'm
21:56
bringing with me from kubernetes Land that's pretty standard to have a slash Health Z path um that you can hit to see if your
22:02
server is live and running so that's the purpose of this Handler it should just respond if the server is alive and
22:09
running and everything's good okay so let's go ahead and run the server and make sure it's doing what we'd expect so
22:14
go build and Dot slash RSS AG that starts up the server and then we can open up thunderclient
22:22
and now instead of making a request to the root which we'd expect to get a 404 from we'll do slash V1 slash Health Z
22:30
and make that get request and we get the 200. now here's the weird thing if I change
22:36
this to a post request and I make that I actually still get a 200 but that's
22:42
not really Our intention the health Z endpoint should really only be accessible by get request so I'm going
22:47
to make an update here rather than using the v1rander.handlefunk I'm going to use v1router.get and this will scope the
22:55
Handler to only fire on get requests okay with that let's go ahead and rebuild our server
23:01
and check again host should fail method not allowed perfect but the get request should still
23:09
work so we have a nice helper function for responding with arbitrary Json now I want one for responding with arbitrary
23:16
error messages so let's do function respond with error it will look very similar but instead of taking a payload
23:22
which is an interface it will take a message string and this function is basically just going to format that
23:27
message into a consistent Json object every single time
23:33
okay I would say if the code is greater than 499 we're going to log a message and
23:40
that's because uh error codes in the 400 range are client-side errors so we don't
23:46
really need to know about them it just means someone's using our API in a weird way but we do need to know whenever
23:51
we're serving we're responding with a 500 level error code because that means we have a bug on our end and we should
23:57
probably go fix it so we'll do log dot print line responding with 500 level error
24:06
and we'll just tack the message itself on there okay cool after we do that logging we'll
24:12
use the respond with Json function but we'll be responding with a specific
24:19
structure of Json so let's go ahead and Define that as a struct so type
24:24
um error response is a struct and has one field error
24:30
which is just a string and we'll add this Json tag to just say this the key
24:36
that this should Marshal to is error so in go we typically take a struct and add
24:43
these Json reflect tags to it to specify how we want this json.martial function
24:49
or on the other side the json.unmarshall function uh to kind of convert this
24:55
struct into a Json object so in this case we're saying I have an error field
25:01
it's a string and I want the key for the field to be error so this struct will
25:06
Marshal into a Json object that looks kind of like uh like this
25:13
error you know something went wrong
25:20
right it wouldn't have uh actually wouldn't have that but it would look like that
25:25
okay and we'll see that in just a second okay so now we get to respond with Json
25:31
we pass in the response writer a cut the same code that we were given and then
25:36
we'll just respond with an error response and the error message will be the
25:42
message that we were given okay let's let's do another Handler
25:48
so here we can do V1 router dot get
25:54
create an error endpoint and oh I need I need an actual Handler so we'll create a
26:01
new one called Handler
26:08
Handler error and we'll respond with an error instead of passing in an empty
26:14
struct we'll say something went wrong
26:20
and we'll respond with a 400 status code client error right
26:26
okay now we can hook up this error Handler here it will only work on get requests that
26:33
seems reasonable and basically it's just going to call that respond with error function so it'll be a good way to test
26:39
that okay let's go ahead and rebuild the server
26:47
oh what do we screw up routing pattern must begin with Slash
26:52
ah let's go fix that so you can see here we've got slash
26:58
Health Z slash V1 we need to start these with a slash it's just the way the chai route or the
27:04
QI router works cool um let's go open up the Thunder client
27:10
and send a request do the slash error Handler
27:16
cool we get the 400 bad request status code and this is that Json body
27:22
um so every single time that we need to return an error from our server now we can just use this function and it will
27:28
always use this consistent error format which is great because we can throw this in our documentation and just tell all
27:33
of the users of our API hey this is what you should expect when something goes wrong now that we have a little bit of
27:40
our boilerplate set up I'm going to take the opportunity to commit all of this to get so that I don't lose it
27:46
um I will say that I generally recommend committing the vendor folder so you can
27:52
think of the vendor folder kind of like the node modules folder if you're familiar with JavaScript land and in JavaScript you would never commit it
27:58
it's way too big but in go we typically don't have all that many dependencies so
28:03
it's actually perfectly fine to commit the vendor folder in most scenarios and I'd even recommend it so I'm going to go
28:10
ahead and add that and commit it we'll say boilerplate or HTTP server
28:20
complete for this project we're going to use postgres as our SQL database it's a
P3. Postgres database
28:26
production ready database in fact it's the one I used to build boot.dev you're
28:31
going to need to install postgres on your local machine make sure that the postgres server is up and running and
28:38
you have a client installed that you can use to make kind of one-off SQL queries
28:43
against it I have detailed instructions on how to do all of that in the text instructions for this project over on
28:49
boot.f so again go check those out if you need to figure out how to install postgres locally and get a postgres
28:56
client up and running on your machine I use PG admin so that's what you'll see me using in this tutorial
29:03
so if you followed those instructions then you should have a postgres server running on your local machine and a
29:08
postgres client installed again I use PG admin that's what you're seeing here on
29:13
the screen okay so because postgres is running locally I Have This localhost Server
29:20
here in PG admin that I've connected to again that's the postgres server running on my own machine and under databases I
29:27
have kind of the built-in postgres database but I want to create a new database that we're going to use for
29:32
this project so in this case I'm just going to name it RSS AG
29:38
and we'll create that database and then here within the RSS AG database
29:44
as long as the uh kind of icons are gold then you're connected and everything is
29:50
working at least up to this point let's run a quick query against the database just to really make sure everything's
29:56
working so I'm right clicking here on the RSS AG database and I'm going to click query tool
30:03
and from this tool I should be able to just write some raw SQL so I'm going to go ahead and do a select
30:09
version this should just return the current version of postgres that I'm using
30:14
I'm on version 14.7 and as long as you're on something 14.7 or newer you should be good to go
30:22
now it's important to keep in mind here that PG admin is just a client for interacting with an SQL database right
30:29
we're able to write raw SQL code here and run it against our database server
30:34
if you think about it kind of in an analogous sense PG admin is basically just the same thing as the Thunder
30:40
client where the Thunder client is a client for running one-off HTTP requests
30:45
against our server PG admin is a client for running one-off SQL requests or SQL
30:51
queries against our database directly next we're going to install two command line tools that will allow us to work
30:58
with SQL databases from our go code much easier now these aren't fully fledged
31:04
orms if you're familiar with that term these are kind of lightweight libraries that allow us to work with SQL databases
31:09
using the standard library and just sort of streamline the process for us the first one is called sqlc and again you
31:16
can find all of these commands in the text instructions over on boot Dev so be sure to be following along over there
31:22
but we're going to use the go install command to go grab sqlc and install it
31:28
into our command line once that's done you should be able to just run sqlc version to make sure it's working
31:36
next we'll install Goose the same way so go install and then the installation
31:42
path for goose again that link is over in the text instructions and then you
31:48
can make sure that goose is installed working correctly by typing Goose Dash version the great thing about sqlc and
31:55
goose is that they work based on Raw SQL there's no kind of fancy query language
32:00
that's unique to those tools we can just write SQL queries and we're going to store all of that in our repository so
32:06
I'm going to create a new folder which is called SQL and in there I'll create a new directory called schema and this is
32:13
where we'll store all of our table definitions or more specifically our migrations so uh we'll start with a
32:20
users table and the way Goose works is it runs the migrations in order so we're going to start with a 0 0 1 migration
32:28
and we'll call it users.sql from a very high level the way that
32:34
database migrations work is they have an up and a down statement so for example here we're creating a users table the up
32:41
statement will just create a new users table and the down statement will delete that same table so any down statement
32:48
should just undo the operation of the up statement and that just makes it easy to roll back changes to our database schema
32:55
if we ever need to the goose command line tool Works based off of SQL comments so we'll start with a comment
33:02
dash dash plus goose up
33:08
and dash dash plus goose down and then anything we type here will be
33:15
considered an up migration and anything here will be a down migration so let's start with the up migration it's going
33:22
to be create create table users and the first field will just be called
33:28
ID it'll be a uuid a universally unique identifier I prefer uuids to integer
33:35
primary keys for a number of reasons um I'll link a blog post down in the description below
33:41
um and that's just going to be a primary key next we're going to need a created at
33:47
which is a time stamp not null but we must have must have it
33:53
created at must have an updated at same thing
34:00
and then a user will also have a name and we'll just make that a text field again
34:07
let's say that's not null I need to remember to terminate my SQL statements with a semicolon and for the
34:13
down by migration it's pretty simple we'll just drop the table so drop table users all right let's run our migration
34:20
but first we're going to need to be able to connect to our local database from our program and from our command line so
34:27
very first thing is we'll need a DB URL
34:32
and we'll set it equal to the URL that we use to connect to our local postgres
34:37
server so this isn't to connect to PG admin this is the same connection string that PG admin uses to connect to the
34:44
database server we want to go directly to the database so it's going to look something like this postgres
34:49
is the protocol so colon slash slash again this is just a URL and then we
34:55
have the authentication part which in my case is Wags Lane because that's the
35:01
user on my machine and then colon and then password if you have a password for
35:07
your local database this is where it goes I actually did not set one up because it's just my local database
35:14
and it's going to be at localhost colon 5432 which is the standard port for
35:22
postgres and the last part of the URL is just going to be the database name that you created so in my case I believe it
35:28
was RSS AG o okay so your url will should look very similar to this with maybe you know the
35:35
username the database name something like that could be could potentially be swapped out on your machine
35:40
okay to run our migration here I'm going to copy I'm going to copy this database
35:47
URL and then I'm going to CD into this directory so CD SQL schema
35:55
and then from here I can run goose postgres so I'm telling I'm telling Goose that
36:01
hey I'm using a postgres database and then I'll paste in my connection string and type up so this will run the up
36:08
migration a nasty error here turns out I forgot some commas
36:13
we need to separate all of these field names with commas cool save that file let's try again
36:21
so we got OK z001 users.sql no more migrations so that should have run let's
36:28
check PG admin to make sure that it works so now over in PG admin under my RSS AG database I should be able to come
36:34
into the schemas tab the tables tag and I can see here that I
36:41
now have two tables Goose DB version so this is an automatic table created and managed by goose and then I've got the
36:48
users table that I just created let's go ahead and do a select star from users
36:53
and we should just be able to see those column names come back now let's make sure that the down migration works as
37:00
well go ahead and run the exact same thing but this time down instead and you can
37:05
see that it down migrated the same file now over in PG admin if I right click on
37:11
tables and click refresh you'll see the user's table is gone and this query should fail now
37:16
okay so let's re-up migrate to get that database table created again and then the interesting thing about migrations
37:22
is you can rerun the same up migration and you won't get any errors because Goose knows that you're already migrated
37:28
up to the most recent version of your migrations now it's time to write a
37:34
query so we're using sqlc to handle our queries and Goose to handle our migrations so to get sqlc set up we need
37:41
to create a new file in the root of our project called sqlc.yaml I'm going to paste in this
37:47
configuration here basically it's just telling sqlc what version we're using what database engine we're using and
37:54
where we're going to store our queries the raw SQL for our queries are going to
37:59
live in the SQL directory under a new subdirectory called queries we've specified that here right and here
38:06
I'm going to create a new file I'm just going to call it users.sql
38:12
and again this is where the SQL will live and the way sqlc works is that it takes the SQL statements and it
38:19
generates go code Type safe go code that matches the SQL every sqlc query starts
38:27
off with a comment that starts with its name so name we'll do a create user
38:32
statement and it returns one record so we're saying I want a new function
38:38
called create user oops and it's going to return one user that statement will
38:44
be insert into users ID created Created at updated at and
38:53
name values dollar sign one
39:00
dollar sign to dollar sign three dollar sign four okay so what's this nonsense
39:06
right in sqlc each dollar sign number is interpolated with
39:13
the parameters for the function so this statement will create a new function called create user with four parameters
39:20
and the first parameter will go in right here the second one the third one the fourth
39:25
one Etc so it allows us to create queries that take arguments as input and
39:30
then we'll end the query with just return returning returning star
39:37
okay we want to create a new user and return that record right we expect one record
39:43
back now let's use sqlc to actually generate the go code for this query we
39:50
always run sqlc from the root of our package rather than within the queries directory itself and the reason that
39:57
works is because we have this sqlc.yaml file at the top level okay so if
40:03
everything was written correctly we should be able to do sqlc generate and what happens is it goes and reads
40:11
that query and it also reads our table definitions
40:16
which we've specified here right SQL schema so it
40:21
knows the shape of our tables and it knows the query we want to create and it
40:26
can go automatically generate all of this go code in the internal slash database package
40:32
now we need to actually use the database in our go code so here in main.go I'm
40:37
going to create a new struct called API config and it's going to hold a connection to a database
40:45
now this database dot queries type is actually exposed by that code that we
40:50
generated using sqlc so you can poke around through this package and kind of get familiar with the generated code we
40:56
never manually update this code that's generated by sqlc it's completely managed by sqlc we're just going to
41:03
write raw SQL to generate this code okay next thing we need to do is import
41:09
our database connection so here in dot EnV we have our DB URL
41:16
and we need to grab that and pull it into our application we're also going to
41:22
need to disable SSL mode so SSL mode equals disable and it just this just
41:28
tells our code hey we don't need to be connecting to our local database using
41:34
encryption we kind of trust our local database so we'll parse that as a string so I'll
41:41
do DB URL and if the database URL
41:47
is not found then we'll we'll report a message uh we'll log an error message and exit
41:54
after that we need to actually connect to the database so the go standard library has a built-in SQL package we
41:59
can do sql.open the driver name that we'll be using is just postgres
42:07
and then we can pass in the connection string and this will return a new connection and an error
42:14
and again if there's an error we'll just go ahead and log a message
42:19
and exit
42:24
can't connect to database now this is kind of a weird quirky thing
42:31
about how go handles databases but we actually need to import a database driver into our program but
42:38
we don't actually need to call anything from it so the sqlc docs mention this but basically we just need to include
42:44
this line at the top of our program and we do need to import it so I'll do a go
42:49
get on that lib PQ and we'll import it using that
42:55
underscore just to say include this code in my program even though I'm not calling it directly
43:01
okay with that there now we should be able to create a new API config
43:07
and let's just call it API CFG
43:13
and it takes as one of its Fields a DB
43:18
where am I at I think I scrolled too far
43:24
DB oh and I should probably go mod tidy and go mod vendor
43:32
so that I stop getting weird errors in my in my vs code
43:38
okay uh this API CFG takes a database.queries but if you look here we don't have a database dot queries we
43:44
have an sql.db so we actually need to convert it into a connection to our package and we can do that with database
43:54
dot new and we pass it as input the connection and we'll get back
43:59
queries error here we can say if error it's not equal
44:04
nil
44:15
and pass in the queries to the struct
44:20
oh did I do that wrong maybe this doesn't return an error mismatch two variables but database.new
44:26
returns one okay cool so this actually can't fail it's just a it's just a simple conversion
44:31
we could actually even just do this it's probably easier great now we have an API config that we
44:38
can pass into our handlers so that they have access to our database let's write that create user Handler
44:45
okay so I'm just going to copy paste this Handler Readiness and change it to Handler user
44:52
and we'll update this to say Handler this will be the create user Handler
44:58
now here's the interesting thing about HTTP handlers and go the function signature can't change but we do want to
45:06
pass into this function an additional piece of data we want to
45:12
add this API config so the way we do it is by making this function a method so
45:19
we do API CFG is a pointer to an API config so our function signature Remains
45:27
the Same right it still just accepts these two parameters but now we have some additional data stored on the struct
45:33
itself that we can gain access to and let's hook up this create user Handler in Main
45:41
so we'll add it to the V1 Handler we'll do V1 router Dot
45:47
host we want this to be a post request to slash users
45:52
and we want to use the create Handler create user method which
46:00
we defined on this struct so we can pass in API
46:05
cfg.handler create user and now our Handler will have access to
46:11
um to the database okay cool this Handler needs to take as input a
46:18
Json body it should expect some parameters so we'll do type parameters
46:25
is struct and I think for now we just need a name
46:31
and we need to parse the request body into this struct so we'll do Json dot
46:37
new decoder and r dot body okay and this response or this returns a
46:44
decoder and we do decoder dot decode
46:51
and we want to decode into an instance of the parameter struct so we'll do
46:56
params is an empty parameter struct and we'll decode into
47:04
a pointer two parameters and this returns an error if anything goes wrong
47:13
if there is an error then we should use that Handler function that we made earlier respond with error and say
47:20
something like well we need to pass in W if something goes wrong here it's probably a client-side error right so
47:25
I'm just going to pass in a 400. and we'll say um
47:34
let's see error parsing Json
47:43
cool and then we'll return because we're done at that point if there is an issue okay otherwise we have
47:51
access to a name so we can use our database to create a user so we do API
47:58
cfg.db dot create user now this is the method that SQL C generated for us right
48:07
because it read our create user SQL and it created a create user function for us right and
48:15
it created the parameters as a struct so that's pretty convenient let's see how this works so create user
48:21
accepts a context and some create user param so I'm going to use uh I think
48:27
it's CTX no r dot context there we go
48:34
so that's the context for this request and then we pass in database dot create
48:39
user params this is struct and it should have yep all of our uh types that we
48:47
need to pass into the create user function okay so first things first an
48:52
ID the ID is a uuid um and this is the first point at which
48:58
I think we've needed to use them so we're going to have to import this package so github.com Google slash uuid
49:04
this is a very uh well-known uuid package and go
49:09
we will go get it with that installed we should be able to
49:16
do uuid.new and that will just create a new random
49:22
uuid and if you weren't familiar this is what a uuid looks like in string form
49:27
it's basically just this really long random uh bit of I mean in this case
49:33
represented as text that we can use as a primary identifier for every user so every user will get their own random ID
49:41
cool um I should probably go mod tidy go mod vendor
49:47
it's just good practice every time you install a new package to make sure you vendor it and go mod tidy kind of cleans
49:53
up any unused Imports and resolves some issues there okay uh create an app I'm going to set
50:00
to just time dot now dot UTC it's created now and then updated that
50:06
should represent the last time it was updated uh which would also be now right because we're creating something new
50:12
and the user's name will just be params.name right it's whatever was passed in to this HTTP request in the
50:20
body oh and I just I'm now realizing that I messed up this Json Tech should look
50:25
like this okay cool
50:31
um so create user should probably return an error yep returns a new user and an error
50:38
again if there was an error creating the user we'll want to respond with an error
50:44
and we'll say couldn't create user
50:51
uh 400 seems fine and then we'll actually respond with the
50:56
user object itself okay database dot user and see how that goes well let's let's actually take a
51:03
look and see what does a database.user even look like
51:11
I'm curious yeah so all these I mean all these fields are exported so they should Marshall to Json just fine let's go
51:18
ahead and run this and see and see what we get before we run the code though it looks like I've got a couple little things to resolve here so error is
51:25
already defined there and then here oh I'm messing something up we need to pass that in we need to interpolate that cool
51:32
oh and here as well percent V because those are errors okay
51:38
cool um now let's go ahead and run build and run the code so go build and
51:45
run RSS hack okay server has started so let's go
51:51
ahead and open up the Thunder client and now we'll be sending a post request
51:57
to the user's endpoint and we'll be sending in a Json body
52:05
oh let me grow this just a little bit
52:12
and we need to specify a name I'm going to create a new user called Lane and let's see what happens
52:19
couldn't create user database wagslane does not exist
52:25
it looks to me like I probably messed up my connection string let's go take a look at that
52:30
so here in dot EnV yeah okay we forgot to or I forgot to
52:36
add the name of the database here at the end so we we need to do slash name of
52:43
database all right let's try that again let's rebuild the server
52:48
and resend that request cool we got a 200 response it looks like
52:53
that is a new random ID great of that updated at and the name
53:00
next just to make sure that the record actually was created in the database server itself I'm going to pop back over
53:05
here to PG admin refresh my tables
53:10
there's our users table and rerun this select star from users perfect looks like we've got one record
53:16
in here with all of the data that I would expect now I want to make one more optimization to our code here you can
53:22
see in this Json response that the fee the key names in the Json object are the same as the exported key names
53:30
here in the user struct in the database package now we can't change this struct
53:36
manually again this is generated by sqlc so what I think we should do is instead
53:43
create a new models folder models.go in the main package
53:50
and here we'll create our own user type so type user struct and it will be
53:57
nearly identical so let me go grab this one be nearly identical
54:02
the only difference at this point is that I'm going to add Json tags
54:08
so that I can specify you know what these names should be and we've been
54:13
using kind of this snake case convention so I'm just going to stick with that so updated
54:19
at and name and I'm going to create a function uh we'll say database user to user and it
54:28
will take a DB user
54:33
and return a user and all this does is return a new user struct where we've
54:41
kind of populate it with all of the all the stuff from the database user
54:47
so again the purpose of this is really just I want to own the shape that's being
54:53
returned over the wire right on our HTTP responses and now I have the power to
54:58
configure that easily within within my application so we'll go ahead and just uh
55:04
paste these in here
55:10
okay and then in my user Handler rather than responding with the database user
55:16
I'm going to respond with our user cool
55:22
let's re-run and build that and let's run our query again now
55:28
remember we already have a user in our database so I'm going to
55:33
create a second one let's call this one Rob
55:39
and this time you can see we have those snake case keys
55:45
and again I'm going to go check in PG admin to make sure that Rob is there
55:50
perfect now we've got Lane and Rob and you can see they have a different randomly generated IDs and their
55:55
timestamps are slightly different we're going to be using API keys to authenticate our users on This Server
P4. Authentication w/ API keys
56:02
the nice thing about an API key is Not only is a little more secure than a username and a password but because it's
56:08
so long it also serves as a unique ID for that user so we don't even need a combo of username password we can just
56:15
use the API key in order to kind of uniquely identify people so we need to
56:21
run a migration that adds a new field to the users table so that we can store their API Keys now
56:27
we've already created this migration that creates the users table and we don't want to modify this because it's
56:33
generally a really bad idea to go modifying your existing migrations instead we create a new one so I'm going
56:39
to create a new one and we'll call it zero zero two because we want it to run after the first migration and again
56:45
Goose uses these numbers to know in which order it should run the migrations and we'll call it users API key
56:55
and the migration statements are going to look a little bit different okay so
57:00
the up statement is going to be an alter table so alter table users
57:06
we'll add a column and we'll call the column just API underscore key
57:12
it's going to be a varchar so varchar 64. now the difference between varchar
57:17
and text at least for our purposes here is that the varchar is exactly 64
57:23
characters long so we're saying we want our API keys to be 64 characters long
57:28
and we want those API keys to be unique no two users should have the same API key we also don't want them to be null
57:35
and we're going to set a default a default API key and this is important
57:41
because if we didn't set a default we'd run into an issue when we run this migration remember we already have two
57:48
users in our database currently so if we just try to add a column that has these
57:54
unique not null constraints on it then what's the SQL database going to do how
58:00
is it going to generate new API keys that are unique and not null typically it would just default the new
58:07
um you know you know the field in the existing records to null but because we've said they can't be null and they
58:13
must be unique we need to provide a unique default for every new record or
58:19
for excuse me for every existing record in the database so the default value that we need to add
58:25
again needs to be unique for every person so we're actually going to have to use some random number generation so
58:32
we can generate a unique API key for every user and this is the snippet of code that does that again you can go
58:39
grab this in the text instructions for this project over on boot Dev but let me
58:44
explain basically what it's doing we're generating some random bytes and then we're casting it into a byte array
58:51
and we're using the Sha 256 hash function to get kind of a a fixed size
58:58
output so we're saying take a a big random slice of bytes
59:04
hash them with straw 256 so that we get a fixed size output and then encode it
59:10
in hexadecimal and that's so we get 64 unique hexadecimal characters this one makes
59:16
more sense when we actually run the query and you see what the output looks like and then as far as the down migration
59:23
goes we just need to alter able users and drop
59:28
column API key again down migrations should just undo
59:34
whatever was done in the up migration let's go ahead and run this migration so I'm going to need to change directory
59:39
into SQL schema then we'll run goose postgres
59:46
and then let me go grab the connection string now we do need to peel off this SSL mode disable Goose doesn't need that
59:54
just our code needs that so I'm going to grab the rest of the string and we'll run an up migration
1:00:00
cool so it looks like it ran successfully let's go see if those default values look good
1:00:06
run the select star statement and there you can see the new API keys
1:00:13
so big old hexadecimal strings that uniquely identify every user and should be kept secret by the user because just
1:00:21
the API key is enough to authenticate the user now that we've got our migration and
1:00:26
we've updated our schema we actually need to go update our query right we
1:00:31
need to be creating new API keys for new users so let's go update our create user
1:00:38
function it should now accept an API key as the last parameter
1:00:44
and pass it in here actually you know what
1:00:49
if we do it this way we're basically telling our application code hey you need to go generate an API key in the
1:00:56
same way that we generated it here in our SQL I think it would actually be easier what if we just take this
1:01:03
and plop that in here
1:01:09
right so now we'll use we'll use SQL to generate the new API Keys we don't even need to update the function signature of
1:01:15
our create user function cool so the SQL will just handle the creation of new API Keys every time a new user is created
1:01:22
all right now we should be able to run sqlc generate
1:01:29
insert has more Expressions than Target columns let's see oh right sorry we do still need to pass
1:01:36
in the API key as the column name um the difference is because we are not using dollar sign five our function
1:01:43
signature won't change this got a little confusing I was reading it like a function signature even though it is it is SQL okay run that again
1:01:51
it went off without a hitch you can see it updated a few files in our database package now we should be able to go use
1:01:57
that in our code but before we test our server let's add one more thing let's give us a way to
1:02:04
get a user so we'll create a new function and this one we'll call
1:02:09
get user by API key
1:02:14
and it should return a single row and it's going to be a select statement so select star
1:02:20
from users where
1:02:25
API key equals dollar sign one
1:02:31
and we'll run sqlc generate again to generate the code for that query you can
1:02:36
see it created it here so in our Handler user function we
1:02:42
actually don't need to make any changes to our create user Handler right we didn't change the number of parameters
1:02:48
that we need to pass in for the API key it's handled kind of under the hood by the SQL query
1:02:54
but we do need a new Handler for getting users so let's go ahead and add that
1:03:01
so I'm going to copy paste that do Handler get user by API key
1:03:09
maybe we can just let's just simplify this let's just call it Handler get user now this Handler is going to look very
1:03:15
different this is an authenticated endpoint so in
1:03:21
order to create a user on our API basically to register a new account you don't need to have an API key but if you
1:03:28
want to get your own user information you have to give us an API key first
1:03:33
this isn't going to be the only authenticated endpoint or the endpoint that you can only do if you're logged in
1:03:39
so I think it makes sense to kind of abstract the logic for getting a user by
1:03:46
their API key into a package so under the internal package here where we have
1:03:51
our database code I'm going to create a new package and we'll just call it auth and in there I'll create a new file
1:03:58
we'll call it auth.go and this whole package will just be called auth
1:04:05
now the only function that we care to export in this auth package is going to
1:04:12
be this one called get API key and its purpose get API key it will say it extracts
1:04:18
an API key from the headers of an HTTP request so it's
1:04:26
going to go into the headers look for a specific header and see if it can find the API key if it can it'll return it
1:04:32
otherwise it will return an error now as the authors of This Server we get
1:04:38
to decide what we want the authentication header to look like so I'm just going to say example
1:04:46
let's expect an authorization header so the key of the header will be authorization
1:04:51
and the value will be API key and then sum you know like insert API
1:05:00
key here okay so we're looking for a header of
1:05:05
this format so first let's look and see if we can find a value associated with the
1:05:11
authorization key um we're just using the HTTP standard Library here so we can do headers dot
1:05:18
get authorization and this should return let's see a string okay so the value
1:05:24
associated with this with this header key now if value is the empty
1:05:32
string then we can just say turn empty string and an error say
1:05:38
errors.new no authentication info
1:05:46
about so otherwise we have a valid value so we could do something like this uh
1:05:54
vowels strings dot split
1:05:59
bowel okay so strings dot split takes a string
1:06:05
as input and a delimiter so we're going to say we want to split this string here
1:06:10
right the the value given to us by the authorization header we want to split it
1:06:16
on Spaces okay so next we can say if the length of
1:06:21
vowels does not equal 2 then again we can return an error saying
1:06:28
like you know maybe malformed malformed ah header
1:06:33
right because we're expecting that the value of this key is two like two
1:06:41
specific values separated by a space the first should be API key and the second
1:06:46
should be the API key right the first would just be the string API key and the
1:06:51
second should be the actual API key okay so if the length is wrong then uh
1:06:58
next we should probably check and make sure they typed this incorrectly so we can say if vowels it's zero does not
1:07:05
equal API key malformed auth header
1:07:13
we could say malformed first part of auth header okay otherwise
1:07:21
we can just return thousand one and no error right
1:07:29
because by that point we're sure that all the all this part was correct and
1:07:35
we're extracting just the API key okay what did I screw up here errors.new
1:07:40
oh right you're not supposed to capitalize errors in go that is a a linting error
1:07:48
stylistic error okay cool so now we've got the get API key
1:07:54
function we can go ahead and use this in our get user Handler so let's go ahead and grab that API key
1:08:01
so API key and error auth.getapi key and we pass in the HTTP
1:08:08
headers so r dot header perfect if there is an error let's
1:08:14
handle it we can just respond with an error saying
1:08:21
auth error and 400 we should probably do like a 403
1:08:26
and in fact now that I'm thinking about HTTP codes creating a user probably should be a 201 instead of a
1:08:33
200. like you probably won't run into any issues for using a 200 but 201 is
1:08:39
the created code so it's like a little more correct if you're looking at it
1:08:45
from kind of a restful HTTP standpoint and then 403 is one of these kind of
1:08:50
permissions errors so that should be good okay now that we have an API key we can
1:08:56
use our database query that we created dot getuser by API key
1:09:02
again we'll pass in the requests context and the API key
1:09:08
I haven't really touched on this yet in go there is a context package in the
1:09:14
standard library and basically it gives you a way to track something that's happening across
1:09:20
multiple go routines and the most important thing that you can do with a context is you can cancel it so by
1:09:27
canceling the context it would effectively kill the HTTP request I don't want to go too much into detail on
1:09:34
how all of that works here you could definitely go read up on it but for now just make sure that it's important to
1:09:40
kind of use the current context so every http.request has a context on it and you
1:09:46
should use that context in any calls you make within the Handler that requires a
1:09:52
context just in case uh kind of cancellations happen okay cool uh that returns a user and an
1:10:01
error again if there is an error
1:10:07
Let's do let's use a better uh string here maybe
1:10:15
couldn't get user and this one let's just go with a 404 or
1:10:21
a 400 for now cool um and then we can respond with Json
1:10:29
this time it will just be a 200 code and again we should cast that database user
1:10:35
to the user model that we defined here right with the the nicely formatted Json
1:10:42
tags and that should be good okay let's hook up our get user Handler
1:10:49
to our router so here we'll do V1 router dot get so
1:10:57
same path slash users but we'll be hooking up
1:11:02
the get user Handler to the get HTTP method so again same path different
1:11:07
method all right let's test our new endpoint
1:11:12
um first I'm going to go ahead and rerun sqlc generate because I can't remember if I've done that and then we'll build
1:11:19
and run the server okay with that running let's head over
1:11:26
to thunderclient and first let's create another new user
1:11:33
so Json body let me minimize this a little bit
1:11:39
um let's keep create a new user we'll call them Rand and it'll be a post request that looks
1:11:45
good okay response came back Rand was created and this is rand's ID
1:11:52
ah we screwed something up we're not responding with the API key
1:11:57
let's go update that so here in our model I believe it's
1:12:04
because we're not casting so we need API e
1:12:09
string Json API key
1:12:15
and then here we need to do that conversion so this is just getting dropped because we weren't
1:12:21
we weren't setting the API key anywhere okay uh let's rebuild
1:12:30
and we'll create a new user let's call this one Joe okay cool
1:12:37
so Joe was created and it actually returned the API key perfect now let's
1:12:43
go ahead and I'm going to create a new request this one
1:12:49
also to localhost 8080 slash V1 slash users but this one will
1:12:56
be a get and we're going to add some headers or specifically one header right we're
1:13:01
going to add our authorization header and its value will be API key
1:13:08
and then the actual API key just paste it in there okay let's go ahead and run that
1:13:15
and it's returning Joe it's good to test failure cases too so
1:13:20
let's just see what happens if we update this header and let's just like
1:13:26
let's just make something broken let's just remove a section of the API key and see what happens
1:13:32
cool we get a 400 battery Quest couldn't get user SQL no rows in the result set so essentially user is not found perfect
1:13:39
so we've got our users set up and our authentication system now it's time to actually get to some business logic
1:13:45
right this is an RSS feed aggregator so we need a way to store feeds let's
1:13:50
create a new schema or rather a new migration in our schema folder this will be 003 we'll call it feed
1:13:59
s.sql now this is going to be a create table migration right we want to create
1:14:04
a beads table so we'll do create table feeds
1:14:09
now a feed and the drop will also drop the feed stable a feed has an ID just
1:14:15
like a user it also has a created at and an updated at and it also has a name like all of that is actually very
1:14:21
similar what's unique about a feed is it has a URL which is text as well that is unique
1:14:30
and not null and it also has a user ID
1:14:38
which references sorry it's a user ID is a uuid which references
1:14:46
users ID and we'll also add the on delete Cascade
1:14:54
essentially what this does is it says we have a user ID stored in our feeds table that references the ID of a user in the
1:15:03
users table right so this is this is relational data this is a relational database essentially what this means is if we try
1:15:10
to create a feed for a user ID that does not exist in the users table we'll get an error which is
1:15:17
what we want because we don't want feeds to be able to exist without a user who created them and then this on delete
1:15:22
Cascade bit just says when a user is deleted I want all of the feeds
1:15:28
associated with that user to be deleted automatically it will Cascade and delete all of them and let's run this migration
1:15:35
so I'm going to hop into the SQL schema directory and from here we can run goose postgres
1:15:43
postgres and grab my connection screen again we don't need the SSL mode stuff
1:15:48
for goose grab the rest of it and up
1:15:54
cool now over here in PG admin I can do a select star from feeds and make sure
1:16:00
that that table exists with those fields next we'll need a query to create a new
1:16:05
feed so I'm going to go ahead and copy this queries file update it to feeds and
1:16:10
then we'll delete this one because we don't need it and we'll create a create feed
1:16:17
and insert into feeds ID created updated at name there will not be an API key but
1:16:24
let's see we need a name URL and user ID
1:16:29
name URL user ID we won't be generating an API key here
1:16:37
instead all of these values I think will just be passed in from our application code so one two three four five six we
1:16:45
need six parameters for this function five and six and then we'll just return the
1:16:52
entire feed row after it's done being created and then I'll just navigate back to the root of the project
1:16:59
and run SQL sqlc generate
1:17:04
to create the code for that new query next we're going to create a new Handler that will allow users of our API to
1:17:12
create a new feed here's the thing that Handler is also going to need all of this same logic
1:17:20
that we have in the get user Handler right we'll need to grab an authentication a token or an API key
1:17:27
from the authorization header fetch the user and then use that user in the Handler and rather than copying and
1:17:34
pasting uh this what 10 lines of code into every Handler that's authenticated instead we're going to build some
1:17:40
middleware to kind of dry up the code right um let's go ahead and do that so I'll
1:17:46
create a new file I'm going to call it middleware auth dot go
1:17:52
art of the main package and here we're going to define a new type and it's our own custom type I'm
1:18:00
calling it auth Handler and you'll notice it looks almost exactly like a
1:18:07
regular HTTP Handler the only difference is that it includes a third parameter it
1:18:12
has a user associated with it so if you think about this it makes a lot of sense for any authenticated Handler to accept
1:18:20
three parameters where the third one is the authenticated user now the problem
1:18:26
with this auth Handler type that we created is that it doesn't match the function signature of an HTTP dot
1:18:33
Handler func right those functions with just the response writer and the request
1:18:39
as the only two parameters so what we're going to do is create a new function called middleware auth that works it's a
1:18:46
method on our API config so it has access to the database um but it its job is to take an auth
1:18:53
Handler as input and return a Handler func so that we can you know use it with the QI router
1:18:59
okay let's Implement that the way this function will actually work is we're
1:19:05
going to return a closure so we're returning here a new Anonymous function with that same function signature as
1:19:13
your normal HTTP Handler func the difference is that as we Define this
1:19:18
function we'll have access to everything within the API config so we'll be able to query the database so we can
1:19:25
basically just go rip out the code from our get user Handler and paste it in here
1:19:32
all right we're going to go get the API key from the request well from the
1:19:37
request headers at least and then we can go ahead and grab the user using that
1:19:44
API key all right so we'll have access to the user here in the function
1:19:50
finally all we need to do is run the Handler that we were given
1:19:55
with the response writer the request and the user right so by the time we get to
1:20:01
actually calling the auth Handler we're able to give it an actual user from the database and
1:20:09
this is really great let me show you why so now that the middle middleware auth function exists we can remove all of
1:20:16
this code from the get user Handler we can update the get user Handler to
1:20:22
accept as input
1:20:27
a database user user database.user
1:20:32
look at how clean this function becomes right now it's just now it's literally just one line cool and now to hook it up
1:20:40
you'll notice we have an error over here we just need to call our middleware auth function
1:20:47
API CFG dot middleware off
1:20:52
we call this function to convert the get user Handler into a standard
1:20:58
http.handlerfunk I kind of move fast through that hopefully it all makes sense though basically we're just
1:21:04
calling the middleware auth function first to get the authenticated user and then
1:21:10
we're calling that callback the the get user Handler the nice thing is now we'll be able to reuse that middleware across
1:21:17
many different HTTP Handler functions so now let's create the create feed Handler
1:21:23
I'm going to go ahead and just copy this Handler user change it to Handler feed
1:21:29
and for now we just need a create function so I'll delete this get and we'll call it Handler
1:21:35
create feed and remember it's an authenticated endpoint so we can have it except the
1:21:41
user directly so user database dot user we know who's creating the feed by the
1:21:47
time we get to this function which is awesome all right in order to create create a
1:21:53
feed use our new create feed function which takes create feed params
1:22:00
and create feed params have ID created updated at name that's all the same the
1:22:05
difference is a URL and a user ID so URL
1:22:12
and a user ID which is a uuid which actually exists already on the
1:22:18
user object so we just do user.id okay um what do we want as input we need a
1:22:24
URL so we also are going to want a params.u well
1:22:35
so we want the user that's creating a new feed to be able to just send us a name and a URL and we'll go about
1:22:42
creating the entire uh kind of you know feed object in the database or feed Row
1:22:47
in the database so this is what our parameters should look like uh what error am I running into here cannot use API cfg.create feed
1:22:57
no new variables on left side of that's odd create feed should return a feed
1:23:04
let's take a look at that definition yeah it does return a feed what am I
1:23:10
messing up here oh first of all that shouldn't be a user
1:23:16
that should be a feat okay that's the problem I was over I was trying to overwrite the database.user type with a
1:23:22
feed type that won't work okay so we're creating a feed we're generating a new uid that's great we're
1:23:29
using the current time perfect um this is getting messed up a variable
1:23:35
of type uuid.uid as uuid.no uuid ah okay
1:23:42
I see the problem the create feed params
1:23:47
except a uuid.null uuid that's a problem we don't ever want a u the null uuid
1:23:53
type from the uuid package is a nullable uuid but we don't want it to be nullable
1:23:59
because we expect that every feed will be created by a user so let's go update our
1:24:07
our uh I think it's our is it our migration let's go look schema feeds
1:24:13
yeah user ID uuid not null
1:24:19
cool it doesn't need to be unique a user can have multiple feeds but it should never be null
1:24:24
all right um with that we're actually going to need to go
1:24:31
uh rerun our migration so SQL um schema
1:24:40
and we'll do a down to drop the table down
1:24:46
and then back up to create the new table with the proper schema
1:24:52
and then we should re-run SQL C generate
1:24:58
okay did that work create feed create feed params looks
1:25:03
like that error is gone it's now just a uuid type perfect now at this point we
1:25:09
have a valid database feed I should probably update this error so that it actually says feed and we want to return
1:25:16
it in our HTTP response trouble is remember
1:25:21
we don't want to just directly return the struct let's go create a new model for a feed
1:25:27
so we'll do type feed then I'm going to go just copy
1:25:36
the types from here and we'll use those
1:25:41
and we'll make our own Json tags for the type
1:25:49
user ID URL name
1:25:55
updated at and created that okay and then we'll
1:26:00
want a what is it database database feed to
1:26:08
feed
1:26:17
and we'll return a feed
1:26:31
all of this should be pretty straightforward and again this just gives us more
1:26:37
control right now we in our code that's not generated by SQL C
1:26:42
are able to Define you know what the shape of the response will look like if for whatever reason we
1:26:49
needed to store some data in the database but never wanted to respond with it in our Json API we could make
1:26:55
those changes here in this struct okay so now we've got database feed to feed we'll call that
1:27:02
and now we should be good to go all right let's test out our new Handler so I'm going to rebuild our server and
1:27:10
run it and over in the Thunder client let's see
1:27:16
so we just created a new user Joe so we have our authentication key here or our API key
1:27:23
I'm going to do a new request to HTTP slash localhost
1:27:30
8080 slash V1 slash feeds ah now that I typed this out I realize that we never
1:27:36
hooked it up let's go hook up that feed so speed Handler Handler create feed we
1:27:43
need to go paste this in to the main function
1:27:48
so this one will go under slash feeds because we're creating a resource we're going to use a post request
1:27:55
Handler create feed okay that should be hooked up now let's restart our server
1:28:04
and over here again I need to grab that API key all right
1:28:15
what do we want here post request localhost 8080 V1 feeds
1:28:24
we do need to authenticate again so query headers
1:28:32
then authorization API key
1:28:38
paste in that API key okay uh what do we send in as the body
1:28:45
let's take a look at our Handler again a name and a URL okay
1:28:50
so name now remember this is this is this is not a person this is a feed right and a feed is a URL that kind of
1:28:59
links to an RSS feed out on the internet so this one I'm going to put just lanes
1:29:05
lanes blog and then the URL is going to be
1:29:12
https colon slash wagslane.dev
1:29:18
slash index dot XML so in case you're
1:29:23
not familiar with what an RSS feed is let me just show you really quick um I'm here on my blog wagslane.dev and
1:29:30
if you click RSS up at the top it'll take you to my RSS feed now every RSS
1:29:35
feed will have a different URL it's kind of up to the author of the blog or the
1:29:40
podcast what that feed URL is but you can usually find it by poking around on their website so in my case it's just
1:29:47
wagsly.dev index.xml and it will look something like this if you open it up in a browser
1:29:53
it's basically this structured XML document that describes what each post
1:29:59
on the blog says at least from a high level it'll usually have something like a link to the post maybe a short
1:30:06
description um basic stuff like that again podcasts also work on the same RSS structure so
1:30:12
for testing you can use my blog or if you know of any other RSS feeds out on the web you can use them
1:30:18
so now that I've pasted in that URL uh let's go ahead and create that feed
1:30:29
s got a new URL created that updated at the name and the URL seem to have
1:30:34
persisted correctly and that is the user ID associated with the API key that we
1:30:40
use to create the feed next we're going to add the ability for any user to get all of the feeds in the database this is
1:30:48
not an authenticated endpoint we need new query I'm going to go ahead
1:30:53
and use the same file this query will just be called get feeds
1:30:58
and it will return many rows instead of just one all right and this one will be uh select
1:31:09
star from feeds super super simple query here we're just
1:31:15
going to go grab all the feeds and return them okay from there we should be able to
1:31:21
sqlc generate and let's go hook up that Handler
1:31:27
I'm going to use the same Handler feed file uh but this one will be a little bit
1:31:32
different you're going to
1:31:38
call this one Handler get feeds
1:31:43
it's not authenticated so we don't need to pass in a user and it doesn't even take any parameters right it's just
1:31:50
going to go get all of the feeds so API cfg.db dot get feeds
1:31:56
and it the get feeds function if you remember we just wrote it in SQL it doesn't take any parameters and here it
1:32:03
just returns all the feeds that are currently in the database so this error should be say something like couldn't
1:32:09
get feeds cool um now we need to return all of the
1:32:17
feeds this is not a single feed this is now a slice of database dot feeds so not
1:32:24
only do we need to return them but we need to actually convert them so let's go update our models a little bit let's
1:32:29
create a new function this one will be database feeds to feeds
1:32:37
and the difference is it will accept a slice of database fees and will return a slice of feeds
1:32:44
let's do this slice feed we'll create a new empty slice of feeds and then four
1:32:50
DB feed range DB feeds
1:32:59
feeds equals append feeds
1:33:05
database feeds or database feed and return
1:33:11
Beats sorry not we don't want to append it directly we have to we have to call our conversion function
1:33:17
okay so this function will just iterate over all of the database feeds one by one
1:33:22
converting them into our new feed structure and then returning them cool now here we can use that function
1:33:30
to do that mapping great let's hook up this Handler
1:33:35
so in main.go we'll create a new entry here
1:33:40
this is going to be a get request and it's not authenticated
1:33:49
it'll be Handler get feeds cool let's regenerate I can't remember
1:33:56
if I generated MySQL C so I'll do that couldn't hurt and then we'll build and run the server
1:34:06
okay with that running let's go ahead and create a couple more RSS feeds so here
1:34:13
update the body of my request sorry I'm so zoomed in so
1:34:19
that you guys can see and it just makes it hard um no this was to create users this is
1:34:25
users here's feeds let's let's just add the same URL with a
1:34:32
couple well no we can't we can't add the same URL let's just use some garbage URLs just to test
1:34:45
all right okay that created properly now let's test our new endpoint
1:34:53
this one is going to be a get request to feeds and we don't need to add any
1:34:59
authentication information okay run that
1:35:04
awesome this looks good to me we've got an array at the top level and then two feed objects one for garbage blog and
1:35:11
one's for one for lanes block so it looks like everything's working so we've given users a way to create feeds and a
P5. Many to many relationships
1:35:17
way to query all of the feeds now we're going to give users a way to follow specific feeds so that they can see kind
1:35:24
of an aggregated view of all of the feeds that they care about on the system okay so uh let's go ahead and add a new
1:35:31
migration we need a new table so this will be the fourth migration and
1:35:37
we'll call this new table feed follows and this table is just going to store the relationship between a user and all
1:35:45
of the feeds they're following so it'll be a many-to-many uh kind of table of user IDs to feed IDs
1:35:52
all right the table is going to be called feed follows so create table feed follows every feed
1:35:59
follow like every other record in our database will have an ID created at and an updated at
1:36:05
but its unique Fields will be a little bit different first it's going to need a user ID
1:36:13
which is oh my gosh why can't I type fingers throw the wrong place in the keyboard so
1:36:20
a user ID is a uuid um that can be it doesn't see it doesn't
1:36:25
need to be unique um but it does need to be not null
1:36:32
then we need a feed ID also a uuid not null
1:36:39
and then we're going to create a unique constraint on the combination of user ID
1:36:46
to feed ID so unique user ID
1:36:54
so again this constraint is going to make it so that we can never have two
1:37:00
instances of a follow for the same user feed relationship right you as a user
1:37:06
can only follow a certain feed once you can't follow it twice that doesn't really make sense right so we're gonna
1:37:12
ensure that that's unique also I missed a couple things here the user ID should
1:37:18
reference so references uh the users table
1:37:26
ID field and on delete we'll Cascade so if a user is deleted we're going to
1:37:33
go delete all of the data about what feeds they're following and then this one's going to be very
1:37:40
similar except it references the feeds table with its ID and again if a feed gets
1:37:46
deleted then we'll go delete all of the following data related to that feed
1:37:53
cool okay let's go ahead and run this migration so I'm gonna go back up into
1:38:00
the SQL or say back down into the SQL schema directory and from here
1:38:07
I'll need my connection string do goose postgres
1:38:13
connection string ah I didn't grab the whole thing let's try that again
1:38:19
all of that goose postgres up
1:38:26
cool speed follows databases or feed follows table is there so now we need a way for
1:38:32
users to follow feeds alright let's go ahead and go create that so I'm going to copy and paste the feed Handler file and
1:38:41
we'll call it Handler speed follows
1:38:46
and update this so Handler create feed follow so remember in order for a user
1:38:52
to follow a feed all we need to do is create a new feed follow record with that user feed relationship
1:39:00
okay this is an authenticated endpoint right we so we we need a user and we need them to be authenticated have
1:39:07
passed an API key right and let's see what do we need them to
1:39:12
give us as input I think all we need is a feed ID right they just need to tell us which
1:39:18
feed they want to follow so a feed ID is a uuid
1:39:28
all right and now we should be able to create the oh we never we never made we
1:39:34
never made the SQL query what am I doing what am I doing I'm getting way ahead of myself let's go add that query quickly
1:39:41
so feed follows and to start we'll need a create feed
1:39:48
follow okay what's in a feed follow right got all these
1:39:55
all of these fields and I think yeah we're just gonna have them all passed in
1:40:00
directly that seems like the easiest way so insert into feed follows
1:40:07
ID created that updated at user ID
1:40:12
ID that's five parameters right one two
1:40:18
three four five cool that looks good now I should be able to go back and run
1:40:24
sqlc generate cool now I should have
1:40:31
a create feed follow function with create
1:40:37
feed follow params all right
1:40:43
it accepts a user ID and a feed ID so the user ID is just the author authenticated user the feed ID is going
1:40:49
to be passed in his params right cool couldn't create feed follow
1:40:58
don't need a get Handler quite yet and then we're gonna just need to make make
1:41:03
that uh mapping function as well for read follows
1:41:11
so in our models file I'll create a new feed follow struct and it's going to have a user ID and a
1:41:19
feed ID
1:41:27
and a new function database feed follow new feed follow
1:41:48
all right DB feed follow dot ID
1:42:01
by the way I'm not using GitHub copilot in this video just so that you could
1:42:06
just just so you can see more of my thought process but I typically do use GitHub
1:42:13
copilot and it makes this kind of function just like way faster to write
1:42:19
it would guess this kind of function almost perfectly um so just so you know I I do recommend
1:42:25
those kinds of tools to speed up the development process um I'm just not using it uh right now so
1:42:30
you can see how I think through you know architecting this this application without all the AI prompts getting in
1:42:36
the way okay now we should be able to database feed follow to feed follow and we're
1:42:42
gonna be clear that this is a feed follow not a feed and that goes there
1:42:49
perfect all right let's hook this up so we're going to need to go into main.go
1:42:56
V V1 router dot post because we're creating a resource
1:43:02
slash feed follows and this is an authenticated
1:43:09
authenticated Handler Handler create feed follow okay let's
1:43:16
test this new endpoint so we'll build and run the server
1:43:25
and we'll need a new request this one will be kind of similar it'll
1:43:32
be oops a post request to the feed follows endpoint
1:43:40
and we're going to need to authenticate so let's go grab some authentication
1:43:45
information get users let's go ahead and
1:43:51
send this couldn't get user SQL no result okay I need to figure out what
1:43:56
users I have available to me oh that's right we we changed this API key we wanted it to break
1:44:03
let's go create a new user we'll make a new one called
1:44:10
uh Billy and there's Billy's API key
1:44:17
cool we've got some feeds but our feed follows
1:44:22
need an auth section sorry in the headers
1:44:28
we're doing it manually authorization API key
1:44:35
there's Billy's key and then in the body we need to pass in the ID of the feed
1:44:40
that we want to follow so let's do a get on all of the feeds
1:44:46
and we can follow either of these let's follow Lane's block
1:44:51
there's our feed ID paste that in there and create
1:45:00
amazing new ID for the feed follow there's the user ID the feed ID what
1:45:06
happens if we try to recreate it cool couldn't create feed follow duplicate key value violates unique constraint
1:45:13
that's what we'd expect right we shouldn't be able to follow the same feed multiple times we're already
1:45:18
following it we already have a record uh indicating that we are following it everything appears to be working just
1:45:24
fine next let's give users a way to see all of the different feeds that they are
1:45:29
currently following so we'll do get feed follows
1:45:37
it will return many and the query will be select star from
1:45:45
eat follows where user ID equals dollar
1:45:50
sign one right so get all the feed follows for a given ID
1:45:57
so let's get that hooked up need to run SQL C generate to create that query and
1:46:03
then down here we'll create a new Handler
1:46:08
this Handler will also be authenticated but it's going to be get
1:46:15
feed follows have the user we don't need any parameters here
1:46:21
and we're just going to call get follows and we'll just need to pass in the
1:46:27
user's ID couldn't get feed follow this
1:46:35
cool now we've got a list of feed follows or a slice of feed follows so we're going to need to convert an entire
1:46:42
slice so again down here we'll write this type of a function
1:46:47
going to be database feed follows
1:46:53
to feed follows
1:47:09
all right lots of copying and pasting here feed
1:47:15
follows
1:47:23
okay so now we have a way to convert an entire slice of database feed follows to
1:47:28
our own struct that looks good to me
1:47:35
there feed follows okay cool now we have a Handler for
1:47:40
getting feed follows let's go ahead and update this so we need a new V1 router dot get
1:47:49
slash feed underscore follows
1:47:55
middleware off if feed follow us
1:48:01
Perfect all right let's give that a shot so we'll build and run again
1:48:14
and now let's see so this is um this is the request that we use to
1:48:21
create so let's grab so hard working on such a small screen
1:48:30
let's grab our API key and create a new request
1:48:39
80 V1 feed follows it's going to be a get request it does
1:48:45
need to be authenticated
1:48:51
okay see if that works cool we got the one feedback that we are
1:48:57
currently following finally we need a way to unfollow feeds or to delete feed
1:49:03
follows so let's create a new one new query we'll do delete
1:49:09
feed follow now this one is going to be our first query that doesn't actually return anything
1:49:16
um it's just going to be an execute right we're not returning one record we're not returning many records we're
1:49:22
returning no records we're just going to run a a SQL query so uh it's going to be
1:49:27
delete ROM feed follows
1:49:34
where ID equals dollar sign one and user ID
1:49:41
equals dollar sign two now it's important it's important to point out that we don't actually need the user ID
1:49:49
here for this query to work right the ID is already a unique identifier the
1:49:55
reason I'm tacking on this user ID is because this will prevent someone who
1:50:00
doesn't own a feed follow from trying to unfollow a feed on behalf
1:50:06
of somebody else that makes sense uh if for whatever
1:50:11
reason another user got accessed let's say if if for some reason user B got
1:50:18
access to the feed follow ID of user a if we didn't have this check here then
1:50:25
that user who hijacked a feed follow ID would be able to like unfollow like
1:50:32
force the other user to do an unfollow if that makes sense this ensures that
1:50:38
only the user who actually owns the follow record can execute the unfollow
1:50:43
command hopefully that makes sense okay uh from here let's just go ahead
1:50:49
and generate that and go hook it up to a new endpoint
1:50:58
so we'll do Handler Elite feed follow
1:51:06
now this one's going to be a little different in that it is authenticated but we need
1:51:12
to get a feed follow ID and delete requests so like HTTP delete request the
1:51:17
delete HTTP method they don't typically have a body in the payload it's it's
1:51:23
possible but I would argue it's not super conventional it's a little more conventional to pass the ID in the HTTP
1:51:31
path so it's going to look something like this
1:51:39
you and router dot delete feed follows slash feed follow
1:51:45
ID and then this will be Handler uh Delete feed follow right so we want
1:51:54
the feed follow ID dynamically passed in the path of the request
1:51:59
so the question is how do we grab this feed follow ID in our Handler itself
1:52:04
well the chi router has or Chi I'm never going to say that the proper way the G
1:52:11
router has a I think it's Pat is it URL let's see URL
1:52:17
parameter that's the one uh you are pro URL parameter function where we can pass
1:52:22
in the request and a key and in this case it's going to have to match so feed follow ID matches whatever we type in
1:52:29
here between the open and close brackets okay and that's going to return a string
1:52:36
so this is the feed follow ID string
1:52:41
great we're going to take that and we're going to parse it into a uuid so we'll do uuid.parse
1:52:49
and that will return a feed follow ID and potentially an error
1:52:58
if the error it's not equal nil we'll say couldn't parse feed
1:53:04
Hollow ID and that will be a 400 level error
1:53:10
perfect okay from here we should be able to do API cfg.database dot delete feed follow and
1:53:18
we need to pass in the request context and
1:53:24
feed follow params so database dot V delete feed follow parameters it takes
1:53:32
an ID and a user ID so the ID of the feed follow we just parsed
1:53:38
and then the user ID comes in with that user object because this is an authenticated request
1:53:45
cool and that should return just an error
1:53:52
right oh and it's just it's just given me yellow squigglies because I need to handle the error
1:53:57
I couldn't delete feed follow
1:54:04
perfect uh what do we respond with here I guess we have a couple different options um the simplest thing would just be to
1:54:09
respond with like an empty Json object I guess uh what matters to the client is
1:54:16
probably the 200 Response Code um so we could like for the sake of
1:54:21
Simplicity just so we can use our respond with Json function we'll just return an empty Json object
1:54:26
alternatively maybe we could return an object that says like message you know
1:54:33
unfollow successful or something um but it doesn't matter too much I think
1:54:38
as long as it's a 200 level code we're pretty much good to go okay and that's already been hooked up
1:54:45
so let's go ahead and test it I can't remember if I generated let me
1:54:50
do that again and then we'll restart the server
1:54:55
and take a look okay so this was our endpoint it's
1:55:03
returning the feeds that we're currently following let's go ahead and delete
1:55:08
this feed follow so we need new request
1:55:15
this is going to be a delete request we're going to unfollow a specific ID we're going to unfollow this
1:55:22
we're going to delete this feed follow you'd follow with that with that ID
1:55:28
and the headers we do need to be authenticated as the same person
1:55:37
so authorization
1:55:42
same API key okay let's run that delete we got a 200 response now let's go do a
1:55:50
get and make sure that it's gone yep empty list or empty array we're good to
1:55:57
go okay we've built out the majority of the crud section of our API but we
P6. Aggregation worker
1:56:03
haven't built the most interesting part which is the part of the server that actually goes out and fetches posts from
1:56:09
the different RSS feeds that exist in our database again the whole purpose of this server
1:56:14
that we're building is so that it can keep track of all of these different feeds in the database and then go out
1:56:20
periodically and actually download all of the posts that are on each individual
1:56:26
feed so for example we have a feed for my personal blog post this server will actually go out to my blog every I don't
1:56:34
know 10 minutes and check to see if there's a new blog post to download and
1:56:39
store in the database so the first thing we need to do is update the feeds table
1:56:44
to have one more column we need a new column called last fetched at and it's
1:56:49
just so we can keep track of when we last fetched the posts for a given fee so let's go ahead and add that we'll
1:56:55
need new migration um and it will look kind of like this migration uh it's going to be our fifth
1:57:01
migration so far it's going to be on the feeds table and we're going to be adding the last fetched at
1:57:09
last fetched at field okay so alter table feeds add
1:57:15
column last fetched at
1:57:21
and this one is going to be timestamp
1:57:27
and it will be nullable so we don't need a not null constraint um
1:57:33
in fact that's it um it's okay like we don't need to specify any defaults that should be it
1:57:39
and then as far as the down migration goes we'll just be deleting or dropping
1:57:45
the column from the feeds table okay cool let's run that migration
1:57:54
so goose postgres
1:58:03
perfect so we don't need to update the create feed function we want the last
1:58:10
fetch that field to default to null so no changes are necessary there but we do need a new we do need a new query this
1:58:16
one's going to be called yet next
1:58:22
oh my gosh get next feed to fetch can't type today get another speed to fetch
1:58:27
and it will return a single row and this one should say select star
1:58:33
from feeds order by
1:58:39
last fetched at descending nulls first
1:58:46
limit one okay so we always this the purpose of
1:58:52
this function is to go get the feed that next needs to be fetched like we need to
1:58:58
go get posts for this feed next and the whole idea is
1:59:03
first we want to go find any feeds that have never been fetched before those need to take priority after that if
1:59:10
every feed has been fetched then we want to go find the one that was fetched the longest ago like the farthest in the
1:59:17
past right so we're ordering by last fetched at um nulls first in descending order
1:59:25
actually scratch that we're going to want to do ascending right ascending would put the
1:59:32
lowest the smallest time stamps right the ones further in the past at the top and then Ascend into the present okay so
1:59:41
order by last fetch that ascending nulls first perfect okay just to make sure that my SQL code
1:59:47
is valid we'll generate that looks good okay next we need one more query this
1:59:53
one will be called Mark feed fetched
1:59:58
Arc feed I guess as fetched this is one we'll call after we fetch a feed to say that we fetched it and we'll
2:00:05
return the updated feed okay so it's going to be update
2:00:11
beads set last fetched at equal to now
2:00:18
and update it at also equal to now so we
2:00:23
haven't really gone over this but the updated and created that fields are mostly for auditing purposes it's pretty
2:00:29
standard practice to set these fields on basically every record in an SQL
2:00:34
database just so you can see when they've been created and updated it's kind of again auditing purposes
2:00:40
okay where ID equals dollar sign one
2:00:48
and returning star okay so we update the feeds we set the
2:00:56
last fetch stat and the updated at to the current time for the given ID that looks good to me
2:01:02
let's go ahead and generate that perfect next we need a way to kind of take an
2:01:08
RSS URL or a feed URL and parse it into an actual response body and in this case
2:01:14
we're going to represent it as a struct let me show you what I mean so let's create a new file I'm just going to call
2:01:20
it rss.go and it's going to be part of the main
2:01:25
package and we need a new function and we're going to call it RSS to
2:01:34
or actually let's call it url url to feed
2:01:39
okay and it's going to take as input a URL
2:01:45
which is just a string and it will return a new type so we need to specify the new type type RSS
2:01:53
feed it will return both an RSS feed and
2:02:00
potentially an error if there's something wrong with the request that it's making now that RSS feed struct that we just
2:02:07
created is going to represent basically this giant this giant document here
2:02:12
right so if you go to wagslane.dev index.xml which is a valid RSS feed then
2:02:19
you'll see this giant document and really you can think of RSS as just a
2:02:24
structured data in XML format and XML is just kind of like crappy Json so the way
2:02:30
we parse XML in go is very similar to The Way We parse Json let me show you
2:02:36
what I mean I've done the Dirty Work of scanning all of the valid values in that
2:02:42
big RSS document and I found that basically these are the keys
2:02:49
um for the RSS entries in my blog so RSS
2:02:54
is kind of a standardized set of keys within XML and basically what I'm saying
2:02:59
is these are the keys that we care about right at the top level of an RSS feed we expect a channel key right in the XML
2:03:07
document and we expect a channel to have a title a link a description a language and then a slice of items and then items
2:03:15
are kind of these nested objects that each have their own title link descriptions and publication dates right
2:03:21
and each item is a new blog post and if you're asking how I came up with those
2:03:27
names of all of the different Keys it's because I went and looked here in this document I saw okay at the top level we
2:03:33
have a channel right and then we have this entry with a title a link a
2:03:38
description right so I just kind of manually looked through this document and found all the stuff that I wanted to
2:03:44
parse out so let's fill in the rest of this URL to feed function so first we're
2:03:49
going to need an HTTP client I'm just creating a new client using the HTTP Library um we'll set it to a timeout of
2:03:55
10 seconds if it takes more than 10 seconds to fetch an RSS feed we don't want that feed anyway probably broken
2:04:02
okay uh then we can use that client to make a get request to
2:04:07
the URL of the feed and that's going to return an HTTP response and potentially an error
2:04:16
if there's an error we'll just return let's just do
2:04:21
um for cons for ease of use I'm going to make this a pointer to an RSS feed so we can just return nil and the error
2:04:29
cool um if everything's okay then we're going to defer a close
2:04:35
on let's close on the rest sorry it's not it's not the close function it's
2:04:41
resp.body Dot close okay and then after that we want to get
2:04:48
all of the data from the response body so it's going to be io.read all we have
2:04:54
to read everything from resp dot body and that comes back as a
2:04:59
slice of bytes and an error
2:05:06
okay this slice of bytes we want to read into
2:05:11
this RSS feed so dealing with XML in go is very similar to dealing with Json and
2:05:19
go it's actually going to be XML Dot unmarshall pass in the data and a
2:05:28
pointer to where we want to unmarshall today so actually I need to create an empty struct we need RSS feed
2:05:35
is an empty RSS feed struct then we'll unmarshall into that location
2:05:43
in memory that will return an error
2:05:48
if everything goes well then we can just return the new populated RSS feed
2:05:59
now as I type this out I'm already kind of dissatisfied with this pointer solution I don't think that needs to be
2:06:05
a pointer I think we should just return empty structs either way would work I think this is a
2:06:12
little cleaner though because it means the user of this function us right we'll
2:06:18
get an actual RSS feedback and not a pointer to an RSS feed okay let's go ahead and test this really
2:06:23
quick I'm just going to do a little kind of hacky thing just right at the top of main I'm going
2:06:30
to call URL to feed and give it the URL of
2:06:35
um my blog so wagslane.dev slash index dot XML
2:06:42
that should return a feed and an error right and then if error not equal nil
2:06:52
blog dot fatal error otherwise I want to just print out
2:06:57
fmt Dot print line let's just print out the whole feed it'll be disgusting but at
2:07:04
least we'll get to see if it kind of worked okay let's build and run that
2:07:18
there we go cool so if you kind of scroll through this you'll see it looks like I mean
2:07:25
there was no errors and then it looks like we properly at least you know at first glance looks
2:07:32
like we properly filled out that struct it's kind of just dumping all of the data so now that we've done a sanity test on
2:07:39
our URL to feed function let's go write the actual scraper create a new file just call this
2:07:46
scraper.go again in the main package
2:07:51
okay um the scraper is a long running job so this scraper function is going to be
2:07:58
running in the background as our server runs so we'll let's name it something like start scraping
2:08:06
and let me split up these parameters so we can really see so we can actually see what uh
2:08:12
what we're dealing with here so we'll take three inputs a connection to the database
2:08:18
um a number of concurrency units I guess the best way to think about this is how
2:08:24
many different go routines we want to do the scraping on and then how much time we want in
2:08:31
between each request to go scrape a new RSS feed cool and it shouldn't return anything
2:08:37
because this is going to be a long running job now because this worker this scraper is going to be running in the
2:08:43
background of our server I think it's really important that we have good logging that kind of tells us what's
2:08:48
going on as it's happening so when we start scraping I'm going to do a little
2:08:53
log message here so log dot printf we'll say
2:08:59
scraping on percent the go routines
2:09:06
every percent s duration
2:09:12
as in the concurrency and the time between requests cool after that we need to figure out
2:09:20
like how we're going to make our requests on this interval and there's a
2:09:25
really cool mechanism in the standard library and go called a ticker so we can create a new
2:09:31
ticker uh using the standard Library so time dot new ticker and we give it a
2:09:37
duration in this case time between requests and it responds with a ticker
2:09:45
and then we can use a for Loop to execute the body of the for Loop
2:09:52
every time a new value comes across the ticker's channel so the ticker Has a
2:09:58
Field called C which is a channel where every kind of let's say that you know time
2:10:05
between requests was set to one minute in that case every one minute a value would be sent across the channel so by
2:10:11
using this syntax here we could say run this for Loop every one minute and the
2:10:17
reason I'm passing in an empty initialize and an empty middle section
2:10:22
to the for Loop is so that it executes immediately the first time so the very first time we get to line 17 the body of
2:10:29
the for Loop will request will will fire immediately and then it will wait for the for the uh
2:10:37
interval on the ticker if that makes sense if we just did this oops if we just did what is it for range
2:10:46
ticker.c uh then it would actually wait for the minute up front but I want to do it once immediately it'll make it easier
2:10:52
to debug and work with now at this point I realize that I've made a mistake the purpose of this concurrency
2:10:58
parameter here is to you know indicate to the start scraping function how many
2:11:04
go routines we want to use to go fetch all of these different feeds and the
2:11:09
whole point is that we can fetch them at the same time so that means that each time that this ticker fires we need to
2:11:15
be potentially go you know going it out to the internet to fetch 10 20 30
2:11:20
different RSS feeds and download all of their blog posts at the same time which
2:11:26
means we'll actually need to be able to grab a a multiple number of feeds we'll
2:11:32
need to grab more than just one at a time so rather than get next feed to fetch let's change this to get next
2:11:38
feeds to fetch and we'll have it return many and then rather than limiting to
2:11:43
one let's limit to dollar sign one so we can actually pass in how many feeds we
2:11:50
want as a parameter to this function okay then we should be able to regenerate that
2:12:00
and we should oh we're not even using the function yet so okay so that was actually the perfect time to do that
2:12:07
okay let's fill out the body of this for Loop so every Interval Timer tweet request we want to go grab the next
2:12:14
batch of feeds to fetch so we can just call that function that we just wrote database.getnextfeeds to fetch
2:12:21
it takes a context and a limit so the first thing we'll just use context dot
2:12:27
background so again I haven't gone into a ton of detail on the context passage but
2:12:32
basically context dot background is like the global context it's what you use if you don't have access to a scoped
2:12:39
context like we do for our individual HTTP requests okay so that'll work for now and then we
2:12:46
also need to pass in a limit so we'll just cast into 32 and the limit or the
2:12:52
sorry the concurrency and that should return some feeds and an
2:12:59
error if there's an error we should probably
2:13:06
print something
2:13:18
now notice I'm continuing here that's because this function should always be
2:13:24
running as our server operates like there's no time in which
2:13:29
we want this function to ever stop so if I returned here that would be a problem it would actually stop scraping
2:13:35
completely just because maybe I don't know our database connection was down temporarily so for now we're just going to log and continue now that we have a
2:13:43
slice of feeds let's write some logic that goes and fetches each feed individually and importantly fetches
2:13:49
each individually at the same time so we're going to need a synchronization mechanism I'm going to use a weight
2:13:56
group so the standard library has this awesome thing called a sync.weight group
2:14:03
then we can iterate over all of the fees so four feed range feeds
2:14:13
okay so the way that the weight group works is anytime you want to spawn a new
2:14:18
go routine within the context of the weight group you do a weight group dot add
2:14:24
and you add some number to it so here I'm iterating over all of the feeds that we want to fetch on individual go
2:14:31
routines and I'm going to add one to the weight group then at the end of the loop I can do a
2:14:37
weight group dot weight and within the loop I can spawn a new go
2:14:44
routine so we're going to go do some function in fact I guess I should just name it kind of what we'll be doing uh
2:14:52
let's call it scrape feed go scrape feed
2:14:57
and here we're actually going to pass the weight group in as one of the parameters
2:15:02
and within scrape feed
2:15:08
takes a weight group which is a pointer to a sink weight group
2:15:16
within this function we'll defer a weight group dot done okay
2:15:23
so what happens here basically we're iterating over the the all of the
2:15:29
feeds on the same go routine as the you know the start scraping function so on
2:15:35
the main go routine on the main go routine we're adding one to the weight group for every feed right
2:15:43
so say we had a concurrency of 30 we would be adding 30 to the weight group
2:15:49
now we'll be spawning all of these separate go routines as we do that and
2:15:54
when we get to the end of the loop we're going to be waiting on the weight group for 30 30 distinct
2:16:02
calls to done so done effectively decrements the counter by one right done
2:16:09
decrements the counter by one so we're adding one every time we iterate over the slice and then we're calling done
2:16:16
when we're done actually scraping the feed so what this does is it allows us to call scrape feed at the same time 30
2:16:24
times so we go spawn 30 different go routines to scrape 30 different RSS feeds
2:16:30
and when they're all done line 35 will kind of execute and will
2:16:36
move past that until they're like before they're done we'll be blocking on line 35 which is what we want to do because
2:16:42
we don't want to continue on to the next iteration of the loop until we are sure
2:16:47
that we've actually scraped all of the feeds so we've sort of stubbed out the scrape
2:16:52
feed function right now doesn't do anything other than call weight group dot done let's actually go scrape some feeds
2:16:58
so it's going to need access to a database connection and it's also going to need a specific
2:17:05
feed to go fetch so feed is a data base dot speed
2:17:11
cool and then here we can pass in database and
2:17:17
great the first thing scrape feed should do and and keep in mind we're deferring
2:17:23
the weight group dot done so this will always be called at the end of this function
2:17:28
um the first thing we should do is Mark that we've fetched this feeder that we're fetching this feed so it's going to be database.mark feed as fetched we
2:17:36
can just use the background context again and we need the ID of the feed so feed
2:17:43
Dot ID cool that should return an error if
2:17:49
there was an error I think oh it also Returns the updated feed I don't think we care about the updated feeds I think we can ignore that
2:17:56
say if error it's not equal nil now keep in mind we're not returning anything from this function remember we're
2:18:02
calling it on a new go routine so there's nothing to return here if there's an error instead we'll just log there was an issue
2:18:18
and return nothing and next we need to actually do the heavy lifting which is to go out and scrape the feet so we
2:18:24
already wrote Our URL to feed function let's just use that so URL to feed and we'll pass in the
2:18:32
feed dot URL and we should get back an RSS feed and an error
2:18:40
and if error does not equal nil
2:18:48
error fetching feed
2:18:55
and we'll return there otherwise we need to do some
2:19:01
logging so in the future what we'll do is instead of iterating over all of the
2:19:08
items in the RSS struct that we get back and just printing them to the console we'll be saving them into the database
2:19:15
but for now just so that we can test our scrape feed function
2:19:20
um we're just going to log log all of this to the console so we're going to log each individual post or rather that
2:19:26
we found a post and then how many posts we found the last thing we need to do is
2:19:32
go hook up this start scraping function to our main function so that it actually starts okay so start scraping takes
2:19:40
database concurrency actually I'll just open this up my screen's too small to be working in two tabs at the same time
2:19:46
okay um we're going to need to call it before listen and serve because remember this
2:19:52
is where our server kind of blocks and waits forever for incoming requests so we should probably call it I don't know
2:19:58
right here seems like a good spot
2:20:04
so um it's just it's just a function right yeah it's not a method takes database
2:20:09
concurrency and time between requests okay so we go go start scraping remember we
2:20:15
want to call it on a new go routine so it doesn't interrupt this main flow because remember start scraping is never
2:20:21
going to return it's a long running function this is an infinite for Loop okay
2:20:28
it needs a database connection so we'll actually need to save this in a new variable so that we can use it in
2:20:36
the API config and in the start scraping function next we need the concurrency
2:20:42
um let's just start with 10. seems good and then time between requests let's do time
2:20:47
dot minute perfect I went ahead and added a second RSS feed
2:20:54
so let's go ahead and check the database and see what feeds we have currently so I've got Lane's blog and the boot Dev
2:21:01
blog so there's two now remember when you're setting a concurrency of 10 so we should definitely be able to fetch both
2:21:07
of these blogs at the same time on the first iteration of that Loop if we say deployed this to production and allowed
2:21:14
users to start creating feeds maybe we'd get up to 100 200 400 different feeds in
2:21:19
here then we'd only be fetching 10 at a time right just so we understand how that mechanism works but for now this
2:21:24
should be good enough to test I'm going to update our logs just a bit so we can see which blog
2:21:31
each post is from so found post item.title on feed
2:21:38
.name okay let's go ahead and run that
2:21:49
found posts the properties of pointers and go on feed boot Dev blog perfect kind of scroll up we should be able to
2:21:56
see some the boot Dev Blog has way more blog posts than my personal blog here it
2:22:03
is here's some Lanes blog stuff okay so that looks like it's working we should be able to move on to the next step now
2:22:08
where we'll actually save these blog posts into the database rather than just
2:22:14
logging the titles to the console we're going to need a new table in our database so let's start there
P7. Viewing blog posts
2:22:20
we'll call it posts so the purpose of this table is to store
2:22:25
all of the posts that we are fetching from all the different uh from all the different RSS feeds
2:22:31
okay um what am I doing this is this is queries we need to start with migration
2:22:37
so let's grab this we'll make it zero zero six
2:22:42
posts dot SQL all right Goose up is going to be create
2:22:48
table posts what goes in a posts table
2:22:53
um we are going to need an ID to create that and update it that pretty much never changes
2:22:58
um what else does a post have well it has a title right title
2:23:06
um is that text Dot null that makes sense uh posts also typically have a description
2:23:12
text I'm going to allow that one to be nullable I think it's okay if a post is missing its description
2:23:18
hosts also typically have a published at date published
2:23:23
at which is a time stamp um should we allow that one to be nullable
2:23:28
no let's make that non null what else does a post have it has a URL
2:23:35
the URL should be not null for sure
2:23:40
um I'm also going to say that it should be unique unique I don't think we ever want to store the
2:23:48
same Post in the database twice there's no point right if we have a post
2:23:54
I don't see why we would need it a second time so let's let's go ahead and make that one unique and then lastly let's just put in a feed
2:24:01
ID and the feed ID is going to be a uuid that uh
2:24:09
we're going to want it to be a reference of references feeds ID
2:24:17
right and that's going to be does that need to be unique
2:24:22
it doesn't need to be unique but it should be not null we should always have the feed ID of a post and let's here put
2:24:30
on delete Cascade if we delete a feed we'll Cascade and delete all of its posts and I forgot to put the type here
2:24:36
so URL text not null unique okay that looks good to me let's go ahead and run our migration so it's going to be we
2:24:44
need a CD into SQL schema and run goose postgres
2:24:54
we'll need this connection string
2:25:00
up perfect next we'll need a couple of queries to
2:25:05
interact with this table the first one is going to be just a way to create a post so let's do posts and we'll do
2:25:13
create oops free post return one thing
2:25:20
and we'll just kind of be inserting a bunch of stuff I think let's take a look at the post table so we've got ID
2:25:26
created at updated let me just grab this so I don't forget it
2:25:35
okay so insert into posts ID created at updated at
2:25:42
we also have we have a lot of stuff so I'm actually going to start spacing this out a little bit
2:25:48
create it at updated at it's going to be a title
2:25:54
and a description
2:26:02
publish that URL and a feed ID
2:26:10
okay and the values one two three four how many do we got
2:26:16
eight five six
2:26:22
seven eight returning star
2:26:28
straightforward right insert into posts all of these fields no fancy logic that should be good okay
2:26:35
let's run sqlc generate add that function to our internal
2:26:41
package and then we just need to go use it so down in the scraper now instead of
2:26:46
just logging all of these posts to the console let's save them to the database
2:26:52
I'm going to leave this log message it just says feed blank collected blank posts found so that'll just log all of
2:26:59
the different feeds we're collecting but each individual post I think it's wasteful and kind of busy to log
2:27:04
everything so we're not going to do that instead we'll just call DB dot create post
2:27:10
context.background and what does it take create postparams
2:27:16
database dot create post params okay cool I kind of like how sqlc
2:27:21
breaks down the parameters into um into a struct makes it pretty simple to
2:27:27
work with okay um we've got an ID created that updated that okay ID is just going to be a new
2:27:35
ID
2:27:40
created that updated that that'll just be time.now dot UTC
2:27:53
what's next title description idle
2:28:02
item.title [Music] I just realized oh I just realized now
2:28:07
that I'm not putting the field names kind of embarrassing
2:28:15
okay title description
2:28:22
do they not have a description what does an item have let's take a look at an item
2:28:29
items have title link description yeah it does have a description what am I messing up here
2:28:36
can I use item.description variable of type string as sql.null string ah right okay
2:28:41
so we need to do sql.null string a null string has the string itself and
2:28:46
whether or not it's valid so we just put in the string and then we say it is valid string dot
2:28:53
valid true although actually this is a problem right this is a problem because if
2:29:00
item.description is blank if it's an empty string we're going to be putting in an empty string and saying that it's
2:29:06
there even though it's not so let's not do this let's do something a little different
2:29:13
let's go description it's a new sql.nil string
2:29:24
why is why is that giving me trouble oh because I'm doing it within the call to create post do it right here
2:29:31
okay so we'll create a new sql.null string and then we'll say
2:29:37
if item dot description does not equal the empty string
2:29:42
then we get to set description dot string equal to item.description
2:29:49
and description dot valid equals true
2:29:54
and then we'll use the description here does that make sense so if if the item's
2:29:59
description is blank then we'll set the value to null in the database effectively
2:30:05
um otherwise we'll create the valid description entity
2:30:11
okay next we need a published at let's see we've got an item dot Hub date
2:30:18
which is a string okay so we're gonna need to parse that string to parse that date there is a Time dot parse function
2:30:26
in the standard library and we're going to use this RFC one one two three Z
2:30:31
layout so this is the layout that I'm using on the boot Dev blog and on my blog to be more robust and support all
2:30:38
of the different publishing formats for all the different blogs that we want to scrape we'd probably need to make this
2:30:44
logic a little bit more robust but for now I'm just going to say we're parsing it
2:30:50
this way if it's not that way I guess we take a hike okay if there's an error so if error does not
2:30:58
equal nil we can say log dot print line
2:31:06
didn't parse date
2:31:11
sent V with air Cent V and we'll pass in the actual Pub date
2:31:20
and an error and that's going to be a printf oops cool and then if that's an issue
2:31:27
we'll just continue so if we don't get a valid time uh then we'll just we'll just log it log in move on
2:31:34
okay so published at pass in that I shouldn't use single name variables like
2:31:39
this let's do Hub at
2:31:44
okay what else do we got a URL and a feed ID so URL
2:31:50
it's just going to be the item.link and feed ID
2:31:56
so we have access to the feed ID here we do we have the feed right because we
2:32:01
passed in the feed here e dot ID
2:32:09
now db.createpost does return an error so we need to handle that error
2:32:16
what am I screwing up here oh it also Returns the post itself I don't think we care about the new post
2:32:22
though I think all we care about is if it failed so if error equal nil
2:32:35
go to create post with error okay let's give that a shot okay build
2:32:42
and run now remember we're expecting this time to get logs that just say that the blogs
2:32:49
were collected so 21 posts from Lane's blog 321 posts from the boot Dev blog
2:32:55
were collected I'm going to go ahead and kill the server and let's check PG admin
2:33:00
so now if we select star from Posts you should see a bunch of stuff in here
2:33:06
IDs created at titles descriptions published at dates
2:33:12
awesome this is looking fantastic we scroll down to see how many rows
2:33:17
there are 342 that looks right to me now I think we have an issue here let me
2:33:23
show you what I mean if we run this again so remember we've scraped both of the feeds and pulled in all of the posts
2:33:29
so if I rerun my server at this point yeah we're getting all of these issues
2:33:36
failed to create posts duplicate key value violates unique constraint posts URL key right now this makes sense we
2:33:42
didn't want to store duplicate posts in our database so we have a unique constraint on the post URL which means
2:33:48
when we go try to recreate the posts it fails because we already have the posts in our database
2:33:54
so let's do a little string a string detection so that we don't log this crap
2:34:00
every time this happens because this isn't really an error this is expected Behavior so we can do something like if
2:34:09
strings dot contains air dot error
2:34:15
uh duplicate duplicate e
2:34:24
then do we continue what are we doing here then we continue
2:34:30
otherwise we'll log the air so we're only going to log the error if it's not
2:34:35
a duplicate key error okay so let's run that again and make sure we don't get
2:34:41
those errors perfect we have one last feature to add to our
2:34:47
RSS aggregator we need a way for users to be able to get a list of all of the newest posts from the feeds that they're
2:34:55
following so we'll need a new query we can call this one
2:35:02
get posts for user and it will return many posts
2:35:08
now let's think about this query for a second it's a little more complex than the other queries we've done in that I think we need to do a join so we have
2:35:14
our posts table right posts have IDs created ad updated
2:35:19
at but importantly they have a feed ID so we know what feed every Post in the
2:35:25
database belongs to and we also have a feed follows table that tells us which feeds an individual
2:35:32
or user is following so if we join those two tables together right if we take all
2:35:38
of the feed follow information kind of join it to the posts table then we should be able to filter by all of the
2:35:45
feeds that a user is actually following so what does that look like you do select
2:35:52
host dot star so everything from the post table
2:35:58
ROM posts join feed follows
2:36:05
on oops on post dot feed ID
2:36:12
equals feed follows dot feed ID okay so this adds essentially all the
2:36:19
feed follow information to our like the virtual table for this query right we're
2:36:24
joining those two tables together so now we should be able to filter um the way we want Okay so we've we've
2:36:31
joined them together where hosts dot wait post no where feed
2:36:38
follows dot user ID equals dollar sign one
2:36:45
okay so we join the tables and then we filter all the posts down or rather the
2:36:50
entire table down by the specific user ID so all of the posts that belong to
2:36:56
feeds that the user is not following should at this step get trimmed out
2:37:02
then we can order by let's do Post dot published at descending so give us the
2:37:09
newest stuff first and we'll limit by a configurable amount
2:37:14
so dollar sign too cool let's go ahead and try to generate that
2:37:22
looks like it worked let's see if uh see if when we run the actual application it does what we expect let's hook that
2:37:28
query up to a new endpoint so I'm here in the users file that seems like a
2:37:34
reasonable place we'll do Handler get posts for a user
2:37:40
it will be an authenticated endpoint so we'll need that that user data but here
2:37:46
we're going to call DB dot or sorry API config
2:37:51
dot DB dot get hosts for user and then we can pass in
2:37:56
the request.context and user dot ID
2:38:02
oh we also need I think a limit oh get no no no sorry we take we take
2:38:09
database dot get post for user params right because we had multiple parameters here so we'll need the user's ID and a
2:38:15
limit the user dot ID and for now let's just say a limit of 10. and that will
2:38:20
return to us some posts and potentially an error if we get an error
2:38:27
we'll respond with it couldn't get posts
2:38:34
otherwise we need to return the posts themselves now we should go create
2:38:40
a special posts model right so that we get our our own tag so type post struct
2:38:48
and go mostly copy this from our internal
2:38:54
host model wherever it ended up here it is
2:39:02
okay that's Json tags
2:39:16
goodness typing is hard
2:39:35
now here's an interesting thing this sql.null string is not something
2:39:41
that we're going to want to use in this struct because this is a struct that
2:39:47
Marshals to Json the null string object is a nested struct so if we marshaled it
2:39:53
directly to Json we would actually get description as a Json key and then string as a Json key and then valid as a
2:40:00
Json case would be a little nested object there that's pretty bad user experience because Json natively
2:40:06
supports kind of null in the sense that you can just omit the key or use like
2:40:12
the actual value null so what we're going to do is instead do a pointer to a
2:40:18
string and the way Json marshalling in go works is if you have a pointer to a string and
2:40:24
it is nil then it will Marshal to what you'd expect in Json land which is that null value
2:40:31
okay and then published at
2:40:37
URL and feed ID
2:40:43
okay did I miss anything let me look over this really quick that's looking good
2:40:51
and next we need the conversion so we'll do
2:40:57
database post to post
2:41:31
now this one gets a little hairy right we probably need to do some logic here so we'll say
2:41:38
bar description pointer to a string
2:41:43
and then if DB post dot description dot valid
2:41:49
then we'll set description equal to the address of dbpost.description.string
2:41:58
then we can just directly use the description variable there
2:42:04
all right published at
2:42:12
what else we got URL and feed ID
2:42:21
okay and last but not least we need a way to do it uh do the conversion for an entire slice
2:42:27
bunk DB database posts to posts
2:42:45
and the logic will look pretty much identical to that but I actually think it'll be easier to type
2:42:50
it out so we'll do uh posts slice post
2:43:12
database post post s in the database post
2:43:21
okay there we go a lot of conversion logic there
2:43:28
but now we should be good to just respond with some Json and we can pass
2:43:34
in those posts as database posts cool what am I getting here
2:43:42
struck literally uses unkeyed Fields oh yeah let's not do that we want an ID
2:43:49
and I think it's a limit right what am I messing up user ID
2:43:59
perfect okay let's build and run the server again
2:44:07
oh I realize I made a mistake I need to actually hook this up to something so let's go back into main.go
2:44:14
we need a new endpoint we'll do get
2:44:20
um I don't know user feed now feeds probably a loaded term in this
2:44:26
uh in this application we should say uh let's just do posts and it's going to require middleware so
2:44:34
middleware off and API CFG Dot
2:44:40
Handler get posts for user okay so get slash posts it's an authenticated
2:44:45
endpoint Perfect all right let's rebuild and run that
2:44:50
okay opening up thunderclient
2:44:57
first we need to check well actually let's just grab some auth
2:45:03
information so um this clearly has some off
2:45:12
let's grab that API key create a new request HTTP colon vocal host
2:45:22
V1 posts headers close that
2:45:29
authorization API key okay so
2:45:34
if I make that request I'm getting back no posts now we know we have posts in the database but my my user that I'm
2:45:41
currently logged in as is not following anything all right I'm getting back the empty
2:45:47
array when I when I check my feed follows um but I can check which feeds Exist by
2:45:55
running this API request so let's grab this feed let's grab the
2:46:01
Wags Lane feed and let's go follow that one so we'll post to feed follows here
2:46:08
this feed ID okay so now I should be following let's
2:46:13
check my feed follows great I'm following them so now if I go
2:46:18
get my posts
2:46:23
there we go I should just be getting posts
2:46:29
from the wagslane.dead block perfect let's try following
2:46:36
let's try following um the other one so
2:46:43
where is it feed follows no feeds let's try following the boot Dev blog as
2:46:49
well so post feed follows
2:46:56
send that check my feed follows now I'm following both now if I go get my posts
2:47:04
perfect we're seeing stuff from the boot Dev blog that's it thank you for sticking with me through
2:47:11
all of this mess we've created an amazing blog aggregator that will actually work pretty darn well at scale
2:47:19
you could run this thing uh you know over a long period of time collect millions of blog posts and it would do
2:47:27
pretty well I hope you had a ton of fun with this project I do want to remind you that this is a server right we've
2:47:35
kind of been running it stopping it restarting it but at the end of the day you can just turn it on ADD new feeds
2:47:42
and follows and interact with it directly and it will once a minute go out and collect all of those blog posts
2:47:49
so you could just keep this running on a Raspberry Pi in your house to aggregate you know blog posts podcasts all that
2:47:57
kind of stuff um I will point out that we have done a bit of happy path programming so happy
2:48:03
path programming is when you're not necessarily handling every Edge case out there you're you're handling kind of you
2:48:09
know the thing that you expect to happen most of the time so for example um we only had one type of date parsing for
2:48:16
the published at dates in our RSS feeds but maybe there are RSS feeds out there that use a different date format and
2:48:23
will fail to parse them um so one way that you could extend this project would be to just add a ton of
2:48:29
new RSS feeds and make sure that you deal with the issues as they come up make sure you improve the logging so you
2:48:36
can see the issues when they come up anyways I hope you had a ton of fun with this project and that you learned
2:48:42
something I just want to remind you that we do have an entire back-end learning path
2:48:47
over on boot.dev in go lay so if you liked this project if you liked this course and are looking for some more
2:48:53
content definitely go check out boot.dev we also published a lot of different
2:48:58
ways that you could potentially extend this project to make it cooler for example maybe you add a front end or a
2:49:05
command line application that interacts directly with the API so that you don't need to use a manual client like
2:49:10
thunderclient every time that you want to interact with your posts and then I also just want to remind you before I go
2:49:16
that you can find me on Twitter at wagslane or on YouTube at boot.dev
2:49:23
definitely go subscribe to our YouTube channel as well thank you again to free codecamp for allowing us to publish this
2:49:29
course and this project walkthrough I hope you enjoyed it and I'll see you in the next one
